{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "uv pip install tilus torch \"cuda-python<13\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2mUsing Python 3.12.6 environment at: /usr/local\u001b[0m\r\n",
            "\u001b[37m\u280b\u001b[0m \u001b[2mResolving dependencies...                                                     \u001b[0m\r\u001b[2K\u001b[37m\u2819\u001b[0m \u001b[2mResolving dependencies...                                                     \u001b[0m\r\u001b[2K\u001b[37m\u280b\u001b[0m \u001b[2mResolving dependencies...                                                     \u001b[0m\r\u001b[2K\u001b[37m\u2819\u001b[0m \u001b[2mResolving dependencies...                                                     \u001b[0m\r\u001b[2K\u001b[37m\u2819\u001b[0m \u001b[2mtilus==0.1.1                                                                  \u001b[0m\r\u001b[2K\u001b[37m\u2819\u001b[0m \u001b[2mtorch==2.8.0+cu129                                                            \u001b[0m\r\u001b[2K\u001b[37m\u2819\u001b[0m \u001b[2mcuda-python==12.9.2                                                           \u001b[0m\r\u001b[2K\u001b[37m\u2839\u001b[0m \u001b[2mcuda-python==12.9.2                                                           \u001b[0m\r\u001b[2K\u001b[37m\u2839\u001b[0m \u001b[2mhidet==0.6.1                                                                  \u001b[0m\r\u001b[2K\u001b[37m\u2839\u001b[0m \u001b[2mtabulate==0.9.0                                                               \u001b[0m\r\u001b[2K\u001b[37m\u2839\u001b[0m \u001b[2mfilelock==3.13.1                                                              \u001b[0m\r\u001b[2K\u001b[37m\u2839\u001b[0m \u001b[2mtyping-extensions==4.12.2                                                     \u001b[0m\r\u001b[2K\u001b[37m\u2839\u001b[0m \u001b[2msetuptools==70.2.0                                                            \u001b[0m\r\u001b[2K\u001b[37m\u2839\u001b[0m \u001b[2msetuptools==70.2.0                                                            \u001b[0m\r\u001b[2K\u001b[37m\u2839\u001b[0m \u001b[2msympy==1.13.3                                                                 \u001b[0m\r\u001b[2K\u001b[37m\u2839\u001b[0m \u001b[2mnetworkx==3.3                                                                 \u001b[0m\r\u001b[2K\u001b[37m\u2839\u001b[0m \u001b[2mjinja2==3.1.4                                                                 \u001b[0m\r\u001b[2K\u001b[37m\u2839\u001b[0m \u001b[2mfsspec==2024.6.1                                                              \u001b[0m\r\u001b[2K\u001b[37m\u2839\u001b[0m \u001b[2mnvidia-cuda-nvrtc-cu12==12.9.86                                               \u001b[0m\r\u001b[2K\u001b[37m\u2839\u001b[0m \u001b[2mnvidia-cuda-nvrtc-cu12==12.9.86                                               \u001b[0m\r\u001b[2K\u001b[37m\u2839\u001b[0m \u001b[2mnvidia-cuda-runtime-cu12==12.9.79                                             \u001b[0m\r\u001b[2K\u001b[37m\u2839\u001b[0m \u001b[2mnvidia-cuda-runtime-cu12==12.9.79                                             \u001b[0m\r\u001b[2K\u001b[37m\u2839\u001b[0m \u001b[2mnvidia-cuda-cupti-cu12==12.9.79                                               \u001b[0m\r\u001b[2K\u001b[37m\u2839\u001b[0m \u001b[2mnvidia-cuda-cupti-cu12==12.9.79                                               \u001b[0m\r\u001b[2K\u001b[37m\u2839\u001b[0m \u001b[2mnvidia-cudnn-cu12==9.10.2.21                                                  \u001b[0m\r\u001b[2K\u001b[37m\u2839\u001b[0m \u001b[2mnvidia-cudnn-cu12==9.10.2.21                                                  \u001b[0m\r\u001b[2K\u001b[37m\u2839\u001b[0m \u001b[2mnvidia-cufft-cu12==11.4.1.4                                                   \u001b[0m\r\u001b[2K\u001b[37m\u2839\u001b[0m \u001b[2mcuda-bindings==12.9.2                                                         \u001b[0m\r\u001b[2K\u001b[37m\u2839\u001b[0m \u001b[2mnumpy==2.1.2                                                                  \u001b[0m\r\u001b[2K\u001b[37m\u2838\u001b[0m \u001b[2mtqdm==4.67.1                                                                  \u001b[0m\r\u001b[2K\u001b[37m\u2838\u001b[0m \u001b[2mcuda-pathfinder==1.2.2                                                        \u001b[0m\r\u001b[2K\u001b[37m\u2838\u001b[0m \u001b[2mcharset-normalizer==3.4.3                                                     \u001b[0m\r\u001b[2K\u001b[37m\u2838\u001b[0m \u001b[2midna==3.10                                                                    \u001b[0m\r\u001b[2K\u001b[2mResolved \u001b[1m51 packages\u001b[0m \u001b[2min 590ms\u001b[0m\u001b[0m\r\n",
            "\u001b[37m\u280b\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/0)                                                   \r\u001b[2K\u001b[37m\u280b\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/12)                                                  \r\u001b[2K\u001b[37m\u2819\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/12)                                                  \r\u001b[2K\u001b[37m\u2819\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/12)\r\n",
            "\u001b[2mtomlkit   \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/37.99 KiB                     \u001b[1A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1A\u001b[37m\u2819\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/12)\r\n",
            "\u001b[2mtomlkit   \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/37.99 KiB\r\n",
            "\u001b[2mlark      \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/108.43 KiB                    \u001b[2A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[2A\u001b[37m\u2819\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/12)\r\n",
            "\u001b[2mtomlkit   \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/37.99 KiB\r\n",
            "\u001b[2mlark      \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 14.92 KiB/108.43 KiB                  \u001b[2A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[2A\u001b[37m\u2819\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/12)\r\n",
            "\u001b[2msmmap     \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/23.73 KiB\r\n",
            "\u001b[2mtomlkit   \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/37.99 KiB\r\n",
            "\u001b[2mlark      \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 14.92 KiB/108.43 KiB                  \u001b[3A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[3A\u001b[37m\u2819\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/12)\r\n",
            "\u001b[2msmmap     \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 14.89 KiB/23.73 KiB\r\n",
            "\u001b[2mtomlkit   \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/37.99 KiB\r\n",
            "\u001b[2mlark      \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 14.92 KiB/108.43 KiB                  \u001b[3A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[3A\u001b[37m\u2819\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/12)\r\n",
            "\u001b[2msmmap     \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 14.89 KiB/23.73 KiB\r\n",
            "\u001b[2mtomlkit   \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/37.99 KiB\r\n",
            "\u001b[2mgitdb     \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/61.32 KiB\r\n",
            "\u001b[2mlark      \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 14.92 KiB/108.43 KiB                  \u001b[4A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[4A\u001b[37m\u2819\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/12)\r\n",
            "\u001b[2msmmap     \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 14.89 KiB/23.73 KiB\r\n",
            "\u001b[2mtomlkit   \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/37.99 KiB\r\n",
            "\u001b[2mgitdb     \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 14.92 KiB/61.32 KiB\r\n",
            "\u001b[2mlark      \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 14.92 KiB/108.43 KiB                  \u001b[4A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[4A\u001b[37m\u2819\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/12)\r\n",
            "\u001b[2msmmap     \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 14.89 KiB/23.73 KiB\r\n",
            "\u001b[2mtomlkit   \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 16.00 KiB/37.99 KiB\r\n",
            "\u001b[2mgitdb     \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 14.92 KiB/61.32 KiB\r\n",
            "\u001b[2mlark      \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 14.92 KiB/108.43 KiB                  \u001b[4A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[4A\u001b[37m\u2819\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/12)\r\n",
            "\u001b[2msmmap     \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 14.89 KiB/23.73 KiB\r\n",
            "\u001b[2mtomlkit   \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 16.00 KiB/37.99 KiB\r\n",
            "\u001b[2mgitdb     \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 14.92 KiB/61.32 KiB\r\n",
            "\u001b[2mlark      \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 14.92 KiB/108.43 KiB\r\n",
            "\u001b[2mgitpython \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/203.29 KiB                    \u001b[5A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[5A\u001b[37m\u2819\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/12)\r\n",
            "\u001b[2msmmap     \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 14.89 KiB/23.73 KiB\r\n",
            "\u001b[2mtomlkit   \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 16.00 KiB/37.99 KiB\r\n",
            "\u001b[2mgitdb     \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 14.92 KiB/61.32 KiB\r\n",
            "\u001b[2mlark      \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 14.92 KiB/108.43 KiB\r\n",
            "\u001b[2mgitpython \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 16.00 KiB/203.29 KiB                  \u001b[5A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[5A\u001b[37m\u2819\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/12)\r\n",
            "\u001b[2msmmap     \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 14.89 KiB/23.73 KiB\r\n",
            "\u001b[2mtomlkit   \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 16.00 KiB/37.99 KiB\r\n",
            "\u001b[2mgitdb     \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 14.92 KiB/61.32 KiB\r\n",
            "\u001b[2mlark      \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 14.92 KiB/108.43 KiB\r\n",
            "\u001b[2mgitpython \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 16.00 KiB/203.29 KiB\r\n",
            "\u001b[2mhip-python-fork\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/63.81 MiB                \u001b[6A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[6A\u001b[37m\u2819\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/12)\r\n",
            "\u001b[2msmmap     \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 14.89 KiB/23.73 KiB\r\n",
            "\u001b[2mtomlkit   \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 16.00 KiB/37.99 KiB\r\n",
            "\u001b[2mgitdb     \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 14.92 KiB/61.32 KiB\r\n",
            "\u001b[2mlark      \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 14.92 KiB/108.43 KiB\r\n",
            "\u001b[2mgitpython \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 16.00 KiB/203.29 KiB\r\n",
            "\u001b[2mhip-python-fork\u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 16.00 KiB/63.81 MiB              \u001b[6A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[6A\u001b[37m\u2819\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/12)\r\n",
            "\u001b[2msmmap     \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 14.89 KiB/23.73 KiB\r\n",
            "\u001b[2mtomlkit   \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 16.00 KiB/37.99 KiB\r\n",
            "\u001b[2mgitdb     \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 14.92 KiB/61.32 KiB\r\n",
            "\u001b[2mlark      \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 14.92 KiB/108.43 KiB\r\n",
            "\u001b[2mgitpython \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 16.00 KiB/203.29 KiB\r\n",
            "\u001b[2mhidet     \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/1.22 MiB\r\n",
            "\u001b[2mhip-python-fork\u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 16.00 KiB/63.81 MiB              \u001b[7A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[7A\u001b[37m\u2819\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/12)\r\n",
            "\u001b[2msmmap     \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 14.89 KiB/23.73 KiB\r\n",
            "\u001b[2mtomlkit   \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 16.00 KiB/37.99 KiB\r\n",
            "\u001b[2mgitdb     \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 14.92 KiB/61.32 KiB\r\n",
            "\u001b[2mlark      \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 14.92 KiB/108.43 KiB\r\n",
            "\u001b[2mgitpython \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 16.00 KiB/203.29 KiB\r\n",
            "\u001b[2mhidet     \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 16.00 KiB/1.22 MiB\r\n",
            "\u001b[2mhip-python-fork\u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 16.00 KiB/63.81 MiB              \u001b[7A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[7A\u001b[37m\u2819\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/12)\r\n",
            "\u001b[2mcuda-python\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/7.42 KiB\r\n",
            "\u001b[2msmmap     \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 14.89 KiB/23.73 KiB\r\n",
            "\u001b[2mtomlkit   \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 16.00 KiB/37.99 KiB\r\n",
            "\u001b[2mgitdb     \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 14.92 KiB/61.32 KiB\r\n",
            "\u001b[2mlark      \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 14.92 KiB/108.43 KiB\r\n",
            "\u001b[2mgitpython \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 16.00 KiB/203.29 KiB\r\n",
            "\u001b[2mhidet     \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 16.00 KiB/1.22 MiB\r\n",
            "\u001b[2mhip-python-fork\u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 16.00 KiB/63.81 MiB              \u001b[8A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[8A\u001b[37m\u2819\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/12)\r\n",
            "\u001b[2mcuda-python\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 7.42 KiB/7.42 KiB\r\n",
            "\u001b[2msmmap     \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 14.89 KiB/23.73 KiB\r\n",
            "\u001b[2mtomlkit   \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 16.00 KiB/37.99 KiB\r\n",
            "\u001b[2mgitdb     \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 14.92 KiB/61.32 KiB\r\n",
            "\u001b[2mlark      \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 14.92 KiB/108.43 KiB\r\n",
            "\u001b[2mgitpython \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 16.00 KiB/203.29 KiB\r\n",
            "\u001b[2mhidet     \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 16.00 KiB/1.22 MiB\r\n",
            "\u001b[2mhip-python-fork\u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 16.00 KiB/63.81 MiB              \u001b[8A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[8A\u001b[37m\u2819\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/12)\r\n",
            "\u001b[2mcuda-python\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 7.42 KiB/7.42 KiB\r\n",
            "\u001b[2msmmap     \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 14.89 KiB/23.73 KiB\r\n",
            "\u001b[2mtomlkit   \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 16.00 KiB/37.99 KiB\r\n",
            "\u001b[2mgitdb     \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 14.92 KiB/61.32 KiB\r\n",
            "\u001b[2mlark      \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 14.92 KiB/108.43 KiB\r\n",
            "\u001b[2mgitpython \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 16.00 KiB/203.29 KiB\r\n",
            "\u001b[2mnvtx      \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/532.80 KiB\r\n",
            "\u001b[2mhidet     \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 16.00 KiB/1.22 MiB\r\n",
            "\u001b[2mhip-python-fork\u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 16.00 KiB/63.81 MiB              \u001b[9A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[9A\u001b[37m\u2819\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/12)\r\n",
            "\u001b[2mcuda-python\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 7.42 KiB/7.42 KiB\r\n",
            "\u001b[2msmmap     \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 14.89 KiB/23.73 KiB\r\n",
            "\u001b[2mtomlkit   \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 16.00 KiB/37.99 KiB\r\n",
            "\u001b[2mgitdb     \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 14.92 KiB/61.32 KiB\r\n",
            "\u001b[2mlark      \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 14.92 KiB/108.43 KiB\r\n",
            "\u001b[2mgitpython \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 16.00 KiB/203.29 KiB\r\n",
            "\u001b[2mnvtx      \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 16.00 KiB/532.80 KiB\r\n",
            "\u001b[2mhidet     \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 16.00 KiB/1.22 MiB\r\n",
            "\u001b[2mhip-python-fork\u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 16.00 KiB/63.81 MiB              \u001b[9A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[9A\u001b[37m\u2819\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/12)\r\n",
            "\u001b[2mcuda-pathfinder\u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 16.00 KiB/22.59 KiB\r\n",
            "\u001b[2msmmap     \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 23.73 KiB/23.73 KiB\r\n",
            "\u001b[2mtomlkit   \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 16.00 KiB/37.99 KiB\r\n",
            "\u001b[2mgitdb     \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 30.92 KiB/61.32 KiB\r\n",
            "\u001b[2mlark      \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 30.92 KiB/108.43 KiB\r\n",
            "\u001b[2mgitpython \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 57.30 KiB/203.29 KiB\r\n",
            "\u001b[2mtilus     \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 16.00 KiB/337.42 KiB\r\n",
            "\u001b[2mnvtx      \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 16.00 KiB/532.80 KiB\r\n",
            "\u001b[2mhidet     \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 64.00 KiB/1.22 MiB\r\n",
            "\u001b[2mcuda-bindings\u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 48.00 KiB/11.89 MiB\r\n",
            "\u001b[2mhip-python-fork\u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 735.89 KiB/63.81 MiB             \u001b[11A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[11A\u001b[37m\u2819\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/12)\r\n",
            "\u001b[2mcuda-pathfinder\u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 16.00 KiB/22.59 KiB\r\n",
            "\u001b[2msmmap     \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 23.73 KiB/23.73 KiB\r\n",
            "\u001b[2mtomlkit   \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 32.00 KiB/37.99 KiB\r\n",
            "\u001b[2mgitdb     \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 30.92 KiB/61.32 KiB\r\n",
            "\u001b[2mlark      \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 46.92 KiB/108.43 KiB\r\n",
            "\u001b[2mgitpython \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 57.30 KiB/203.29 KiB\r\n",
            "\u001b[2mtilus     \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 16.00 KiB/337.42 KiB\r\n",
            "\u001b[2mnvtx      \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 16.00 KiB/532.80 KiB\r\n",
            "\u001b[2mhidet     \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 64.00 KiB/1.22 MiB\r\n",
            "\u001b[2mcuda-bindings\u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 76.62 KiB/11.89 MiB\r\n",
            "\u001b[2mhip-python-fork\u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 1.47 MiB/63.81 MiB               \u001b[11A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[11A\u001b[37m\u2819\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/12)\r\n",
            "\u001b[2mcuda-pathfinder\u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 16.00 KiB/22.59 KiB\r\n",
            "\u001b[2msmmap     \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 23.73 KiB/23.73 KiB\r\n",
            "\u001b[2mtomlkit   \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 32.00 KiB/37.99 KiB\r\n",
            "\u001b[2mgitdb     \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 46.92 KiB/61.32 KiB\r\n",
            "\u001b[2mlark      \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 46.92 KiB/108.43 KiB\r\n",
            "\u001b[2mgitpython \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 64.00 KiB/203.29 KiB\r\n",
            "\u001b[2mtilus     \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 32.00 KiB/337.42 KiB\r\n",
            "\u001b[2mnvtx      \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 320.00 KiB/532.80 KiB\r\n",
            "\u001b[2mhidet     \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 64.00 KiB/1.22 MiB\r\n",
            "\u001b[2mcuda-bindings\u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 92.62 KiB/11.89 MiB\r\n",
            "\u001b[2mhip-python-fork\u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 4.06 MiB/63.81 MiB               \u001b[11A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[11A\u001b[37m\u2819\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/12)\r\n",
            "\u001b[2mcuda-pathfinder\u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 16.00 KiB/22.59 KiB\r\n",
            "\u001b[2msmmap     \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 23.73 KiB/23.73 KiB\r\n",
            "\u001b[2mtomlkit   \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 37.99 KiB/37.99 KiB\r\n",
            "\u001b[2mgitdb     \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 46.92 KiB/61.32 KiB\r\n",
            "\u001b[2mlark      \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 62.92 KiB/108.43 KiB\r\n",
            "\u001b[2mgitpython \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 80.00 KiB/203.29 KiB\r\n",
            "\u001b[2mtilus     \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 32.00 KiB/337.42 KiB\r\n",
            "\u001b[2mnvtx      \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 528.00 KiB/532.80 KiB\r\n",
            "\u001b[2mhidet     \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 64.00 KiB/1.22 MiB\r\n",
            "\u001b[2mcuda-bindings\u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 124.62 KiB/11.89 MiB\r\n",
            "\u001b[2mhip-python-fork\u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 5.98 MiB/63.81 MiB               \u001b[11A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[11A\u001b[37m\u2839\u001b[0m \u001b[2mPreparing packages...\u001b[0m (1/12)\r\n",
            "\u001b[2mcuda-pathfinder\u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 16.00 KiB/22.59 KiB\r\n",
            "\u001b[2msmmap     \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 23.73 KiB/23.73 KiB\r\n",
            "\u001b[2mtomlkit   \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 37.99 KiB/37.99 KiB\r\n",
            "\u001b[2mgitdb     \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 46.92 KiB/61.32 KiB\r\n",
            "\u001b[2mlark      \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 62.92 KiB/108.43 KiB\r\n",
            "\u001b[2mgitpython \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 95.89 KiB/203.29 KiB\r\n",
            "\u001b[2mtilus     \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 32.00 KiB/337.42 KiB\r\n",
            "\u001b[2mnvtx      \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 528.00 KiB/532.80 KiB\r\n",
            "\u001b[2mhidet     \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 64.00 KiB/1.22 MiB\r\n",
            "\u001b[2mcuda-bindings\u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 124.62 KiB/11.89 MiB\r\n",
            "\u001b[2mhip-python-fork\u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 8.05 MiB/63.81 MiB               \u001b[11A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[11A\u001b[37m\u2839\u001b[0m \u001b[2mPreparing packages...\u001b[0m (1/12)\r\n",
            "\u001b[2mcuda-pathfinder\u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 16.00 KiB/22.59 KiB\r\n",
            "\u001b[2mtomlkit   \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 37.99 KiB/37.99 KiB\r\n",
            "\u001b[2mgitdb     \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 46.92 KiB/61.32 KiB\r\n",
            "\u001b[2mlark      \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 62.92 KiB/108.43 KiB\r\n",
            "\u001b[2mgitpython \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 95.89 KiB/203.29 KiB\r\n",
            "\u001b[2mtilus     \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 32.00 KiB/337.42 KiB\r\n",
            "\u001b[2mnvtx      \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 528.00 KiB/532.80 KiB\r\n",
            "\u001b[2mhidet     \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 64.00 KiB/1.22 MiB\r\n",
            "\u001b[2mcuda-bindings\u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 124.62 KiB/11.89 MiB\r\n",
            "\u001b[2mhip-python-fork\u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 9.17 MiB/63.81 MiB               \u001b[10A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[10A\u001b[37m\u2839\u001b[0m \u001b[2mPreparing packages...\u001b[0m (1/12)\r\n",
            "\u001b[2mcuda-pathfinder\u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 16.00 KiB/22.59 KiB\r\n",
            "\u001b[2mtomlkit   \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 37.99 KiB/37.99 KiB\r\n",
            "\u001b[2mgitdb     \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 46.92 KiB/61.32 KiB\r\n",
            "\u001b[2mlark      \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 62.92 KiB/108.43 KiB\r\n",
            "\u001b[2mgitpython \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 95.89 KiB/203.29 KiB\r\n",
            "\u001b[2mtilus     \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 32.00 KiB/337.42 KiB\r\n",
            "\u001b[2mnvtx      \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 528.00 KiB/532.80 KiB\r\n",
            "\u001b[2mhidet     \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 64.00 KiB/1.22 MiB\r\n",
            "\u001b[2mcuda-bindings\u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 124.62 KiB/11.89 MiB\r\n",
            "\u001b[2mhip-python-fork\u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 10.19 MiB/63.81 MiB              \u001b[10A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[10A\u001b[37m\u2839\u001b[0m \u001b[2mPreparing packages...\u001b[0m (1/12)\r\n",
            "\u001b[2mcuda-pathfinder\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 22.59 KiB/22.59 KiB\r\n",
            "\u001b[2mtomlkit   \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 37.99 KiB/37.99 KiB\r\n",
            "\u001b[2mgitdb     \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 46.92 KiB/61.32 KiB\r\n",
            "\u001b[2mlark      \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 62.92 KiB/108.43 KiB\r\n",
            "\u001b[2mgitpython \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 111.89 KiB/203.29 KiB\r\n",
            "\u001b[2mtilus     \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 32.00 KiB/337.42 KiB\r\n",
            "\u001b[2mnvtx      \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 528.00 KiB/532.80 KiB\r\n",
            "\u001b[2mhidet     \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 80.00 KiB/1.22 MiB\r\n",
            "\u001b[2mcuda-bindings\u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 172.52 KiB/11.89 MiB\r\n",
            "\u001b[2mhip-python-fork\u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 12.30 MiB/63.81 MiB              \u001b[10A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[10A\u001b[37m\u2839\u001b[0m \u001b[2mPreparing packages...\u001b[0m (1/12)\r\n",
            "\u001b[2mcuda-pathfinder\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 22.59 KiB/22.59 KiB\r\n",
            "\u001b[2mgitdb     \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 61.32 KiB/61.32 KiB\r\n",
            "\u001b[2mlark      \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 62.92 KiB/108.43 KiB\r\n",
            "\u001b[2mgitpython \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 111.89 KiB/203.29 KiB\r\n",
            "\u001b[2mtilus     \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 32.00 KiB/337.42 KiB\r\n",
            "\u001b[2mnvtx      \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 528.00 KiB/532.80 KiB\r\n",
            "\u001b[2mhidet     \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 80.00 KiB/1.22 MiB\r\n",
            "\u001b[2mcuda-bindings\u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 172.52 KiB/11.89 MiB\r\n",
            "\u001b[2mhip-python-fork\u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 12.98 MiB/63.81 MiB              \u001b[9A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[9A\u001b[37m\u2838\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/12)\r\n",
            "\u001b[2mcuda-pathfinder\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 22.59 KiB/22.59 KiB\r\n",
            "\u001b[2mgitdb     \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 61.32 KiB/61.32 KiB\r\n",
            "\u001b[2mlark      \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 62.92 KiB/108.43 KiB\r\n",
            "\u001b[2mgitpython \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 111.89 KiB/203.29 KiB\r\n",
            "\u001b[2mtilus     \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 32.00 KiB/337.42 KiB\r\n",
            "\u001b[2mnvtx      \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 532.80 KiB/532.80 KiB\r\n",
            "\u001b[2mhidet     \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 80.00 KiB/1.22 MiB\r\n",
            "\u001b[2mcuda-bindings\u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 204.62 KiB/11.89 MiB\r\n",
            "\u001b[2mhip-python-fork\u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 14.36 MiB/63.81 MiB              \u001b[9A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[9A\u001b[37m\u2838\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/12)\r\n",
            "\u001b[2mgitdb     \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 61.32 KiB/61.32 KiB\r\n",
            "\u001b[2mlark      \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 62.92 KiB/108.43 KiB\r\n",
            "\u001b[2mgitpython \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 111.89 KiB/203.29 KiB\r\n",
            "\u001b[2mtilus     \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 48.00 KiB/337.42 KiB\r\n",
            "\u001b[2mnvtx      \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 532.80 KiB/532.80 KiB\r\n",
            "\u001b[2mhidet     \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 80.00 KiB/1.22 MiB\r\n",
            "\u001b[2mcuda-bindings\u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 204.62 KiB/11.89 MiB\r\n",
            "\u001b[2mhip-python-fork\u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 14.43 MiB/63.81 MiB              \u001b[8A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[8A\u001b[37m\u2838\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/12)\r\n",
            "\u001b[2mgitdb     \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 61.32 KiB/61.32 KiB\r\n",
            "\u001b[2mlark      \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 78.92 KiB/108.43 KiB\r\n",
            "\u001b[2mgitpython \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 127.89 KiB/203.29 KiB\r\n",
            "\u001b[2mtilus     \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 48.00 KiB/337.42 KiB\r\n",
            "\u001b[2mhidet     \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 80.00 KiB/1.22 MiB\r\n",
            "\u001b[2mcuda-bindings\u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 236.62 KiB/11.89 MiB\r\n",
            "\u001b[2mhip-python-fork\u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 15.35 MiB/63.81 MiB              \u001b[7A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[7A\u001b[37m\u2838\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/12)\r\n",
            "\u001b[2mgitdb     \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 61.32 KiB/61.32 KiB\r\n",
            "\u001b[2mlark      \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 83.68 KiB/108.43 KiB\r\n",
            "\u001b[2mgitpython \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 144.00 KiB/203.29 KiB\r\n",
            "\u001b[2mtilus     \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 48.00 KiB/337.42 KiB\r\n",
            "\u001b[2mhidet     \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 96.00 KiB/1.22 MiB\r\n",
            "\u001b[2mcuda-bindings\u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 428.62 KiB/11.89 MiB\r\n",
            "\u001b[2mhip-python-fork\u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 15.51 MiB/63.81 MiB              \u001b[7A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[7A\u001b[37m\u2838\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/12)\r\n",
            "\u001b[2mlark      \u001b[0m \u001b[32m----------------------------\u001b[2m--\u001b[0m\u001b[0m 99.68 KiB/108.43 KiB\r\n",
            "\u001b[2mgitpython \u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 160.00 KiB/203.29 KiB\r\n",
            "\u001b[2mtilus     \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 64.00 KiB/337.42 KiB\r\n",
            "\u001b[2mhidet     \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 96.00 KiB/1.22 MiB\r\n",
            "\u001b[2mcuda-bindings\u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 636.62 KiB/11.89 MiB\r\n",
            "\u001b[2mhip-python-fork\u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 15.54 MiB/63.81 MiB              \u001b[6A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[6A\u001b[37m\u2838\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/12)\r\n",
            "\u001b[2mlark      \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 108.43 KiB/108.43 KiB\r\n",
            "\u001b[2mgitpython \u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 192.00 KiB/203.29 KiB\r\n",
            "\u001b[2mtilus     \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 64.00 KiB/337.42 KiB\r\n",
            "\u001b[2mhidet     \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 96.00 KiB/1.22 MiB\r\n",
            "\u001b[2mcuda-bindings\u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 860.62 KiB/11.89 MiB\r\n",
            "\u001b[2mhip-python-fork\u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 17.09 MiB/63.81 MiB              \u001b[6A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[6A\u001b[37m\u2838\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/12)\r\n",
            "\u001b[2mlark      \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 108.43 KiB/108.43 KiB\r\n",
            "\u001b[2mgitpython \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 203.29 KiB/203.29 KiB\r\n",
            "\u001b[2mtilus     \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 64.00 KiB/337.42 KiB\r\n",
            "\u001b[2mhidet     \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 112.00 KiB/1.22 MiB\r\n",
            "\u001b[2mcuda-bindings\u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 876.62 KiB/11.89 MiB\r\n",
            "\u001b[2mhip-python-fork\u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 19.03 MiB/63.81 MiB              \u001b[6A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[6A\u001b[37m\u2838\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/12)\r\n",
            "\u001b[2mgitpython \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 203.29 KiB/203.29 KiB\r\n",
            "\u001b[2mtilus     \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 64.00 KiB/337.42 KiB\r\n",
            "\u001b[2mhidet     \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 112.00 KiB/1.22 MiB\r\n",
            "\u001b[2mcuda-bindings\u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 908.62 KiB/11.89 MiB\r\n",
            "\u001b[2mhip-python-fork\u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 20.61 MiB/63.81 MiB              \u001b[5A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[5A\u001b[37m\u2838\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/12)\r\n",
            "\u001b[2mgitpython \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 203.29 KiB/203.29 KiB\r\n",
            "\u001b[2mtilus     \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 64.00 KiB/337.42 KiB\r\n",
            "\u001b[2mhidet     \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 112.00 KiB/1.22 MiB\r\n",
            "\u001b[2mcuda-bindings\u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 924.62 KiB/11.89 MiB\r\n",
            "\u001b[2mhip-python-fork\u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 21.61 MiB/63.81 MiB              \u001b[5A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[5A\u001b[37m\u283c\u001b[0m \u001b[2mPreparing packages...\u001b[0m (7/12)\r\n",
            "\u001b[2mtilus     \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 64.00 KiB/337.42 KiB\r\n",
            "\u001b[2mhidet     \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 112.00 KiB/1.22 MiB\r\n",
            "\u001b[2mcuda-bindings\u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 924.62 KiB/11.89 MiB\r\n",
            "\u001b[2mhip-python-fork\u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 22.30 MiB/63.81 MiB              \u001b[4A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[4A\u001b[37m\u283c\u001b[0m \u001b[2mPreparing packages...\u001b[0m (7/12)\r\n",
            "\u001b[2mtilus     \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 80.00 KiB/337.42 KiB\r\n",
            "\u001b[2mhidet     \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 112.00 KiB/1.22 MiB\r\n",
            "\u001b[2mcuda-bindings\u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 972.62 KiB/11.89 MiB\r\n",
            "\u001b[2mhip-python-fork\u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 23.61 MiB/63.81 MiB              \u001b[4A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[4A\u001b[37m\u283c\u001b[0m \u001b[2mPreparing packages...\u001b[0m (7/12)\r\n",
            "\u001b[2mtilus     \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 80.00 KiB/337.42 KiB\r\n",
            "\u001b[2mhidet     \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 112.00 KiB/1.22 MiB\r\n",
            "\u001b[2mcuda-bindings\u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 2.90 MiB/11.89 MiB\r\n",
            "\u001b[2mhip-python-fork\u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 23.96 MiB/63.81 MiB              \u001b[4A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[4A\u001b[37m\u283c\u001b[0m \u001b[2mPreparing packages...\u001b[0m (7/12)\r\n",
            "\u001b[2mtilus     \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 80.00 KiB/337.42 KiB\r\n",
            "\u001b[2mhidet     \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 128.00 KiB/1.22 MiB\r\n",
            "\u001b[2mcuda-bindings\u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 3.32 MiB/11.89 MiB\r\n",
            "\u001b[2mhip-python-fork\u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 24.98 MiB/63.81 MiB              \u001b[4A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[4A\u001b[37m\u283c\u001b[0m \u001b[2mPreparing packages...\u001b[0m (7/12)\r\n",
            "\u001b[2mtilus     \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 80.00 KiB/337.42 KiB\r\n",
            "\u001b[2mhidet     \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 128.00 KiB/1.22 MiB\r\n",
            "\u001b[2mcuda-bindings\u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 5.05 MiB/11.89 MiB\r\n",
            "\u001b[2mhip-python-fork\u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 26.38 MiB/63.81 MiB              \u001b[4A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[4A\u001b[37m\u2834\u001b[0m \u001b[2mPreparing packages...\u001b[0m (8/12)\r\n",
            "\u001b[2mtilus     \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 96.00 KiB/337.42 KiB\r\n",
            "\u001b[2mhidet     \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 128.00 KiB/1.22 MiB\r\n",
            "\u001b[2mcuda-bindings\u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 5.42 MiB/11.89 MiB\r\n",
            "\u001b[2mhip-python-fork\u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 27.94 MiB/63.81 MiB              \u001b[4A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[4A\u001b[37m\u2834\u001b[0m \u001b[2mPreparing packages...\u001b[0m (8/12)\r\n",
            "\u001b[2mtilus     \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 96.00 KiB/337.42 KiB\r\n",
            "\u001b[2mhidet     \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 128.00 KiB/1.22 MiB\r\n",
            "\u001b[2mcuda-bindings\u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 5.67 MiB/11.89 MiB\r\n",
            "\u001b[2mhip-python-fork\u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 30.03 MiB/63.81 MiB              \u001b[4A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[4A\u001b[37m\u2834\u001b[0m \u001b[2mPreparing packages...\u001b[0m (8/12)\r\n",
            "\u001b[2mtilus     \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 96.00 KiB/337.42 KiB\r\n",
            "\u001b[2mhidet     \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 128.00 KiB/1.22 MiB\r\n",
            "\u001b[2mcuda-bindings\u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 5.78 MiB/11.89 MiB\r\n",
            "\u001b[2mhip-python-fork\u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 31.76 MiB/63.81 MiB              \u001b[4A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[4A\u001b[37m\u2834\u001b[0m \u001b[2mPreparing packages...\u001b[0m (8/12)\r\n",
            "\u001b[2mtilus     \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 96.00 KiB/337.42 KiB\r\n",
            "\u001b[2mhidet     \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 144.00 KiB/1.22 MiB\r\n",
            "\u001b[2mcuda-bindings\u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 5.83 MiB/11.89 MiB\r\n",
            "\u001b[2mhip-python-fork\u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 33.61 MiB/63.81 MiB              \u001b[4A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[4A\u001b[37m\u2826\u001b[0m \u001b[2mPreparing packages...\u001b[0m (8/12)\r\n",
            "\u001b[2mtilus     \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 112.00 KiB/337.42 KiB\r\n",
            "\u001b[2mhidet     \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 160.00 KiB/1.22 MiB\r\n",
            "\u001b[2mcuda-bindings\u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 6.34 MiB/11.89 MiB\r\n",
            "\u001b[2mhip-python-fork\u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 35.09 MiB/63.81 MiB              \u001b[4A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[4A\u001b[37m\u2826\u001b[0m \u001b[2mPreparing packages...\u001b[0m (8/12)\r\n",
            "\u001b[2mtilus     \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 112.00 KiB/337.42 KiB\r\n",
            "\u001b[2mhidet     \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 160.00 KiB/1.22 MiB\r\n",
            "\u001b[2mcuda-bindings\u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 8.15 MiB/11.89 MiB\r\n",
            "\u001b[2mhip-python-fork\u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 35.25 MiB/63.81 MiB              \u001b[4A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[4A\u001b[37m\u2826\u001b[0m \u001b[2mPreparing packages...\u001b[0m (8/12)\r\n",
            "\u001b[2mtilus     \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 131.58 KiB/337.42 KiB\r\n",
            "\u001b[2mhidet     \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 176.00 KiB/1.22 MiB\r\n",
            "\u001b[2mcuda-bindings\u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 9.89 MiB/11.89 MiB\r\n",
            "\u001b[2mhip-python-fork\u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 35.58 MiB/63.81 MiB              \u001b[4A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[4A\u001b[37m\u2827\u001b[0m \u001b[2mPreparing packages...\u001b[0m (8/12)\r\n",
            "\u001b[2mtilus     \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 131.58 KiB/337.42 KiB\r\n",
            "\u001b[2mhidet     \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 176.00 KiB/1.22 MiB\r\n",
            "\u001b[2mcuda-bindings\u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 10.08 MiB/11.89 MiB\r\n",
            "\u001b[2mhip-python-fork\u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 37.51 MiB/63.81 MiB              \u001b[4A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[4A\u001b[37m\u2827\u001b[0m \u001b[2mPreparing packages...\u001b[0m (8/12)\r\n",
            "\u001b[2mtilus     \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 131.58 KiB/337.42 KiB\r\n",
            "\u001b[2mhidet     \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 192.00 KiB/1.22 MiB\r\n",
            "\u001b[2mcuda-bindings\u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 10.30 MiB/11.89 MiB\r\n",
            "\u001b[2mhip-python-fork\u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 39.69 MiB/63.81 MiB              \u001b[4A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[4A\u001b[37m\u2827\u001b[0m \u001b[2mPreparing packages...\u001b[0m (8/12)\r\n",
            "\u001b[2mtilus     \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 144.00 KiB/337.42 KiB\r\n",
            "\u001b[2mhidet     \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 208.00 KiB/1.22 MiB\r\n",
            "\u001b[2mcuda-bindings\u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 10.52 MiB/11.89 MiB\r\n",
            "\u001b[2mhip-python-fork\u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 41.85 MiB/63.81 MiB              \u001b[4A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[4A\u001b[37m\u2827\u001b[0m \u001b[2mPreparing packages...\u001b[0m (8/12)\r\n",
            "\u001b[2mtilus     \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 144.00 KiB/337.42 KiB\r\n",
            "\u001b[2mhidet     \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 224.00 KiB/1.22 MiB\r\n",
            "\u001b[2mcuda-bindings\u001b[0m \u001b[32m----------------------------\u001b[2m--\u001b[0m\u001b[0m 10.84 MiB/11.89 MiB\r\n",
            "\u001b[2mhip-python-fork\u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 43.84 MiB/63.81 MiB              \u001b[4A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[4A\u001b[37m\u2807\u001b[0m \u001b[2mPreparing packages...\u001b[0m (8/12)\r\n",
            "\u001b[2mtilus     \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 176.00 KiB/337.42 KiB\r\n",
            "\u001b[2mhidet     \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 244.60 KiB/1.22 MiB\r\n",
            "\u001b[2mcuda-bindings\u001b[0m \u001b[32m----------------------------\u001b[2m--\u001b[0m\u001b[0m 10.99 MiB/11.89 MiB\r\n",
            "\u001b[2mhip-python-fork\u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 45.88 MiB/63.81 MiB              \u001b[4A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[4A\u001b[37m\u2807\u001b[0m \u001b[2mPreparing packages...\u001b[0m (8/12)\r\n",
            "\u001b[2mtilus     \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 176.00 KiB/337.42 KiB\r\n",
            "\u001b[2mhidet     \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 244.60 KiB/1.22 MiB\r\n",
            "\u001b[2mcuda-bindings\u001b[0m \u001b[32m----------------------------\u001b[2m--\u001b[0m\u001b[0m 11.05 MiB/11.89 MiB\r\n",
            "\u001b[2mhip-python-fork\u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 47.85 MiB/63.81 MiB              \u001b[4A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[4A\u001b[37m\u2807\u001b[0m \u001b[2mPreparing packages...\u001b[0m (8/12)\r\n",
            "\u001b[2mtilus     \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 208.00 KiB/337.42 KiB\r\n",
            "\u001b[2mhidet     \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 320.00 KiB/1.22 MiB\r\n",
            "\u001b[2mcuda-bindings\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 11.75 MiB/11.89 MiB\r\n",
            "\u001b[2mhip-python-fork\u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 49.18 MiB/63.81 MiB              \u001b[4A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[4A\u001b[37m\u2807\u001b[0m \u001b[2mPreparing packages...\u001b[0m (8/12)\r\n",
            "\u001b[2mtilus     \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 272.00 KiB/337.42 KiB\r\n",
            "\u001b[2mhidet     \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 416.00 KiB/1.22 MiB\r\n",
            "\u001b[2mhip-python-fork\u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 49.18 MiB/63.81 MiB              \u001b[3A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[3A\u001b[37m\u2807\u001b[0m \u001b[2mPreparing packages...\u001b[0m (8/12)\r\n",
            "\u001b[2mhidet     \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 496.00 KiB/1.22 MiB\r\n",
            "\u001b[2mhip-python-fork\u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 49.24 MiB/63.81 MiB              \u001b[2A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[2A\u001b[37m\u2807\u001b[0m \u001b[2mPreparing packages...\u001b[0m (8/12)\r\n",
            "\u001b[2mhidet     \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 608.00 KiB/1.22 MiB\r\n",
            "\u001b[2mhip-python-fork\u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 49.24 MiB/63.81 MiB              \u001b[2A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[2A\u001b[37m\u2807\u001b[0m \u001b[2mPreparing packages...\u001b[0m (8/12)\r\n",
            "\u001b[2mhidet     \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 912.00 KiB/1.22 MiB\r\n",
            "\u001b[2mhip-python-fork\u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 49.32 MiB/63.81 MiB              \u001b[2A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[2A\u001b[37m\u280b\u001b[0m \u001b[2mPreparing packages...\u001b[0m (10/12)\r\n",
            "\u001b[2mhip-python-fork\u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 49.41 MiB/63.81 MiB              \u001b[1A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1A\u001b[37m\u280b\u001b[0m \u001b[2mPreparing packages...\u001b[0m (10/12)\r\n",
            "\u001b[2mhip-python-fork\u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 49.42 MiB/63.81 MiB              \u001b[1A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1A\u001b[37m\u280b\u001b[0m \u001b[2mPreparing packages...\u001b[0m (10/12)\r\n",
            "\u001b[2mhip-python-fork\u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 49.60 MiB/63.81 MiB              \u001b[1A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1A\u001b[37m\u280b\u001b[0m \u001b[2mPreparing packages...\u001b[0m (10/12)\r\n",
            "\u001b[2mhip-python-fork\u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 49.84 MiB/63.81 MiB              \u001b[1A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1A\u001b[37m\u280b\u001b[0m \u001b[2mPreparing packages...\u001b[0m (10/12)\r\n",
            "\u001b[2mhip-python-fork\u001b[0m \u001b[32m------------------------\u001b[2m------\u001b[0m\u001b[0m 50.65 MiB/63.81 MiB              \u001b[1A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1A\u001b[37m\u2819\u001b[0m \u001b[2mPreparing packages...\u001b[0m (11/12)\r\n",
            "\u001b[2mhip-python-fork\u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 53.24 MiB/63.81 MiB              \u001b[1A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1A\u001b[37m\u2819\u001b[0m \u001b[2mPreparing packages...\u001b[0m (11/12)\r\n",
            "\u001b[2mhip-python-fork\u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 54.18 MiB/63.81 MiB              \u001b[1A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1A\u001b[37m\u2839\u001b[0m \u001b[2mPreparing packages...\u001b[0m (11/12)\r\n",
            "\u001b[2mhip-python-fork\u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 54.99 MiB/63.81 MiB              \u001b[1A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1A\u001b[37m\u2839\u001b[0m \u001b[2mPreparing packages...\u001b[0m (11/12)\r\n",
            "\u001b[2mhip-python-fork\u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 55.01 MiB/63.81 MiB              \u001b[1A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1A\u001b[37m\u2839\u001b[0m \u001b[2mPreparing packages...\u001b[0m (11/12)\r\n",
            "\u001b[2mhip-python-fork\u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 55.03 MiB/63.81 MiB              \u001b[1A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1A\u001b[37m\u2839\u001b[0m \u001b[2mPreparing packages...\u001b[0m (11/12)\r\n",
            "\u001b[2mhip-python-fork\u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 55.04 MiB/63.81 MiB              \u001b[1A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1A\u001b[37m\u2839\u001b[0m \u001b[2mPreparing packages...\u001b[0m (11/12)\r\n",
            "\u001b[2mhip-python-fork\u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 56.60 MiB/63.81 MiB              \u001b[1A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1A\u001b[37m\u2839\u001b[0m \u001b[2mPreparing packages...\u001b[0m (11/12)\r\n",
            "\u001b[2mhip-python-fork\u001b[0m \u001b[32m----------------------------\u001b[2m--\u001b[0m\u001b[0m 58.61 MiB/63.81 MiB              \u001b[1A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1A\u001b[37m\u2838\u001b[0m \u001b[2mPreparing packages...\u001b[0m (11/12)\r\n",
            "\u001b[2mhip-python-fork\u001b[0m \u001b[32m-----------------------------\u001b[2m-\u001b[0m\u001b[0m 60.63 MiB/63.81 MiB              \u001b[1A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1A\u001b[37m\u2838\u001b[0m \u001b[2mPreparing packages...\u001b[0m (11/12)\r\n",
            "\u001b[2mhip-python-fork\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 63.22 MiB/63.81 MiB              \u001b[1A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1A\u001b[37m\u2838\u001b[0m \u001b[2mPreparing packages...\u001b[0m (11/12)                                                 \r\u001b[2K\u001b[2mPrepared \u001b[1m12 packages\u001b[0m \u001b[2min 2.31s\u001b[0m\u001b[0m\r\n",
            "\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591 [0/0] \u001b[2mInstalling wheels...                                 \u001b[0m\r\u001b[2K\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591 [0/12] \u001b[2mInstalling wheels...                                \u001b[0m\r\u001b[2K\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591 [0/12] \u001b[2mcuda-python==12.9.2                                 \u001b[0m\r\u001b[2K\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591 [1/12] \u001b[2mcuda-python==12.9.2                                 \u001b[0m\r\u001b[2K\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591 [1/12] \u001b[2msmmap==5.0.2                                        \u001b[0m\r\u001b[2K\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591 [2/12] \u001b[2msmmap==5.0.2                                        \u001b[0m\r\u001b[2K\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591 [2/12] \u001b[2mtomlkit==0.13.3                                     \u001b[0m\r\u001b[2K\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591 [3/12] \u001b[2mtomlkit==0.13.3                                     \u001b[0m\r\u001b[2K\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591 [3/12] \u001b[2mnvtx==0.2.13                                        \u001b[0m\r\u001b[2K\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591 [4/12] \u001b[2mnvtx==0.2.13                                        \u001b[0m\r\u001b[2K\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591 [4/12] \u001b[2mcuda-pathfinder==1.2.2                              \u001b[0m\r\u001b[2K\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591 [5/12] \u001b[2mcuda-pathfinder==1.2.2                              \u001b[0m\r\u001b[2K\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591 [5/12] \u001b[2mgitdb==4.0.12                                       \u001b[0m\r\u001b[2K\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591 [6/12] \u001b[2mgitdb==4.0.12                                       \u001b[0m\r\u001b[2K\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591 [6/12] \u001b[2mlark==1.2.2                                         \u001b[0m\r\u001b[2K\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591 [7/12] \u001b[2mlark==1.2.2                                         \u001b[0m\r\u001b[2K\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591 [7/12] \u001b[2mgitpython==3.1.45                                   \u001b[0m\r\u001b[2K\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591 [8/12] \u001b[2mgitpython==3.1.45                                   \u001b[0m\r\u001b[2K\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591 [8/12] \u001b[2mhip-python-fork==6.3.3.540.31.1                     \u001b[0m\r\u001b[2K\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591 [9/12] \u001b[2mhip-python-fork==6.3.3.540.31.1                     \u001b[0m\r\u001b[2K\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591 [9/12] \u001b[2mcuda-bindings==12.9.2                               \u001b[0m\r\u001b[2K\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591 [10/12] \u001b[2mcuda-bindings==12.9.2                              \u001b[0m\r\u001b[2K\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591 [10/12] \u001b[2mtilus==0.1.1                                       \u001b[0m\r\u001b[2K\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591 [11/12] \u001b[2mhidet==0.6.1                                       \u001b[0m\r\u001b[2K\u001b[2mInstalled \u001b[1m12 packages\u001b[0m \u001b[2min 216ms\u001b[0m\u001b[0m\r\n",
            " \u001b[32m+\u001b[39m \u001b[1mcuda-bindings\u001b[0m\u001b[2m==12.9.2\u001b[0m\r\n",
            " \u001b[32m+\u001b[39m \u001b[1mcuda-pathfinder\u001b[0m\u001b[2m==1.2.2\u001b[0m\r\n",
            " \u001b[32m+\u001b[39m \u001b[1mcuda-python\u001b[0m\u001b[2m==12.9.2\u001b[0m\r\n",
            " \u001b[32m+\u001b[39m \u001b[1mgitdb\u001b[0m\u001b[2m==4.0.12\u001b[0m\r\n",
            " \u001b[32m+\u001b[39m \u001b[1mgitpython\u001b[0m\u001b[2m==3.1.45\u001b[0m\r\n",
            " \u001b[32m+\u001b[39m \u001b[1mhidet\u001b[0m\u001b[2m==0.6.1\u001b[0m\r\n",
            " \u001b[32m+\u001b[39m \u001b[1mhip-python-fork\u001b[0m\u001b[2m==6.3.3.540.31.1\u001b[0m\r\n",
            " \u001b[32m+\u001b[39m \u001b[1mlark\u001b[0m\u001b[2m==1.2.2\u001b[0m\r\n",
            " \u001b[32m+\u001b[39m \u001b[1mnvtx\u001b[0m\u001b[2m==0.2.13\u001b[0m\r\n",
            " \u001b[32m+\u001b[39m \u001b[1msmmap\u001b[0m\u001b[2m==5.0.2\u001b[0m\r\n",
            " \u001b[32m+\u001b[39m \u001b[1mtilus\u001b[0m\u001b[2m==0.1.1\u001b[0m\r\n",
            " \u001b[32m+\u001b[39m \u001b[1mtomlkit\u001b[0m\u001b[2m==0.13.3\u001b[0m\r\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Goals\n",
        " Tiles can be thought of as a somewhat high-level introduction to CUDA because it abstracts many things away from you. In fact, it can be even more effective than Triton for learning. In turn, it can allow us to become more familiar with the high-level objectives of CUDA without many of the common problems you may encounter with it.\n",
        "\n",
        "# Note\n",
        "On top, it has a simple installation process and can be quickly walked through in less than a few hours. \n",
        "\n",
        "TILUS does allow for explicit control of the memory hierarchy that Triton does not allow, which can make it more precise. \n",
        "\n",
        "It inherits principles from TVM, even though that project is considered relatively legacy for now. \n",
        "\n",
        "You can access CUDA-specific optimizations, which can be harder to do or less common in Triton. \n",
        "\n",
        "Arbitrary low-precision types are especially useful for future developments. For example, FP4 was just launched in September 2025 for CUDA 13.\n",
        "\n",
        "Overall, it can also be useful for language model inference practitioners who want a deeper dive into various elements used in the day-to-day operations for modeling code and kernels.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Installation\n",
        "In this specific modal environment, you'll have to use cuda-python less than 13. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "import torch\n",
        "import tilus\n",
        "\n",
        "\n",
        "# define the kernel by subclassing `tilus.Script`\n",
        "class MyKernel(tilus.Script):\n",
        "    def __call__(self):\n",
        "        # the configuration settings\n",
        "        self.attrs.blocks = 1  # one thread block\n",
        "        self.attrs.warps = 1  # one warp per thread block\n",
        "\n",
        "        self.printf(\"Hello, World!\")\n",
        "\n",
        "\n",
        "# instantiate the kernel\n",
        "kernel = MyKernel()\n",
        "\n",
        "# launch the kernel on GPU\n",
        "kernel()\n",
        "torch.cuda.synchronize()\n",
        "# sync causes the kernel to wait for all\n",
        "# kernels in streams to complete\n",
        "# it's important, because it basically says:\n",
        "# \"wait for all kernels to finish before proceeding\"\n",
        "# so all subsequent code will be executed only after the kernel has finished\n",
        "# torch.cuda.synchronize()\n",
        "# it's sort of like await.\n",
        "# normally we could just dispatch the cuda execution,\n",
        "# and let it run free like a kid."
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/site-packages/tilus/ir/mfunction/ops.py:156: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  \"\"\"Check whether the multi-function fa covers the multi-function fb.\n",
            "\r[Building] my_kernel: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:07<00:00,  7.80s/it]\r[Building] my_kernel: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:07<00:00,  7.80s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, World!"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "import torch\n",
        "import tilus\n",
        "from tilus import float32, int32\n",
        "from tilus.utils import cdiv\n",
        "\n",
        "\n",
        "class AddOneKernel(tilus.Script):\n",
        "    def __init__(self, block_n, warps):\n",
        "        super().__init__()\n",
        "        self.block_n: int = block_n\n",
        "        self.warps: int = warps\n",
        "\n",
        "    def __call__(self, n: int32, a_ptr: ~float32, b_ptr: ~float32):\n",
        "        self.attrs.blocks = cdiv(n, self.block_n)  # define the number of thread blocks\n",
        "        self.attrs.warps = self.warps  # define the number of warps per block\n",
        "\n",
        "        # get the offset for the current block\n",
        "        offset = self.blockIdx.x * self.block_n\n",
        "\n",
        "        # create two global tensors for input and output, given their pointers\n",
        "        ga = self.global_view(a_ptr, shape=[n], dtype=float32)\n",
        "        gb = self.global_view(b_ptr, shape=[n], dtype=float32)\n",
        "\n",
        "        # load the inputs from global memory into a register tensor\n",
        "        a = self.load_global(ga, offsets=[offset], shape=[self.block_n])\n",
        "\n",
        "        # perform the computation: add 1 to each element in the register tensor\n",
        "        b = a + 1.0\n",
        "\n",
        "        # store the result back to global memory\n",
        "        self.store_global(gb, b, offsets=[offset])\n",
        "\n",
        "\n",
        "def main():\n",
        "    # define the kernel\n",
        "    kernel = AddOneKernel(block_n=128, warps=4)\n",
        "\n",
        "    # create input and output tensors\n",
        "    n = 16\n",
        "    a = torch.arange(n, dtype=torch.float32)\n",
        "    b = torch.empty_like(a)\n",
        "\n",
        "    # launch the kernel\n",
        "    kernel(n, a, b)\n",
        "\n",
        "    print(a)\n",
        "    print(b)\n",
        "\n",
        "\n",
        "main()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[Building] add_one_kernel-d1: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:04<00:00,  4.16s/it]\r[Building] add_one_kernel-d1: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:04<00:00,  4.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13.,\n",
            "        14., 15.])\n",
            "tensor([7.7781e+31, 2.0706e-19, 1.8888e+31, 2.7410e+17, 1.3563e-19, 3.9168e-02,\n",
            "        2.0513e+17, 1.3563e-19, 3.9155e-02, 4.7429e+30, 2.0108e+20, 1.1257e+24,\n",
            "        3.3844e-12, 7.9309e+34, 6.0022e+31, 4.2964e+24])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Basic Language Constructs\n",
        "What can we use in `Tilus`?\n",
        "The `__init__` is responsible for doing compile time setup, so it will 'pre-compute' over the hyper params, you can use it, or you can just record it. In our example, we set numbers such as number of blocks, number of warps. When we call the init method, those will be set (and the standard python init follows.)\n",
        "\n",
        "`__call__` method works for defining actual code that we execute. We need to say: How many 'thread blocks' to launch, so it represents the # of blocks in each dimension, since we are using the 3d block model. It's important to understand this prior to working a lot in tilus.\n",
        "\n",
        "Warps per thread block needs to say the # of warps per thread block. So it is the compilation time constant.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# JIT\n",
        " The common pattern with JIT functions is that at the first run, we will compile them and have an optimized version of them. All kernel parameters must be typed and annotated with type. \n",
        " For JIT annotations and non-JIT annotations, the kinds of valid types can differ.\n",
        "\n",
        "# Objectives\n",
        " In the JIT module, you can do a few main things, including on-demand compilation, where the Script class is compiled into distinct kernels for each setup when it's first invoked with those runtime parameters. It can also perform the popular task of exploring parameter space with autotune decorator, which is similar to Triton. \n",
        "  And this is all with the overarching goal of performance optimization. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "import math\n",
        "import torch\n",
        "import tilus\n",
        "from tilus import float16, float32, int32\n",
        "from tilus.utils import cdiv\n",
        "\n",
        "\n",
        "class Matmul(tilus.Script):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.block_m = 64\n",
        "        self.block_n = 128\n",
        "        self.block_k = 16\n",
        "\n",
        "    def __call__(self,\n",
        "            m_size: int32, n_size: int, k_size: int,\n",
        "            a_ptr: ~float16, b_ptr: ~float16, c_ptr: ~float16\n",
        "    ):\n",
        "        self.attrs.blocks = [\n",
        "            cdiv(m_size, self.block_m),  # the x dimension size of the grid\n",
        "            cdiv(n_size, self.block_n),  # the y dimension size of the grid\n",
        "        ]\n",
        "        self.attrs.warps = 4\n",
        "\n",
        "        offset_m: int32 = self.block_m * self.blockIdx.x\n",
        "        offset_n: int32 = self.block_n * self.blockIdx.y\n",
        "\n",
        "        # create two global tensors `ga` and `gb`\n",
        "        ga = self.global_view(a_ptr, dtype=float16, shape=[m_size, k_size])\n",
        "        gb = self.global_view(b_ptr, dtype=float16, shape=[k_size, n_size])\n",
        "\n",
        "        # create a register tensor `acc` for accumulating the results.\n",
        "        acc = self.register_tensor(\n",
        "            dtype=float32, shape=[self.block_m, self.block_n], init=0.0\n",
        "        )\n",
        "\n",
        "        # iterate over the k dimension in blocks of size `block_k`.\n",
        "        for k in range(cdiv(k_size, self.block_k)):\n",
        "            # calculate the offset for the current block in the k dimension\n",
        "            offset_k = k * self.block_k\n",
        "\n",
        "            # load a block of matrix A and B into register tensors `a` and `b`.\n",
        "            a = self.load_global(\n",
        "                ga, offsets=[offset_m, offset_k], shape=[self.block_m, self.block_k]\n",
        "            )\n",
        "            b = self.load_global(\n",
        "                gb, offsets=[offset_k, offset_n], shape=[self.block_k, self.block_n]\n",
        "            )\n",
        "\n",
        "            # perform the dot product: acc = a @ b + acc\n",
        "            self.dot(a, b, acc, out=acc)\n",
        "\n",
        "        # after the loop, we cast the accumulated result `acc` to float16 type\n",
        "        acc_f16 = self.cast(acc, dtype=float16)\n",
        "\n",
        "        # store it back to the output matrix C.\n",
        "        gc = self.global_view(c_ptr, dtype=float16, shape=[m_size, n_size])\n",
        "        self.store_global(gc, acc_f16, offsets=[offset_m, offset_n])\n",
        "\n",
        "\n",
        "def main():\n",
        "    kernel = Matmul()\n",
        "\n",
        "    for k_size, n_size in [(4096, 4096), (4096, 12288)]:\n",
        "        for m_size in [1, 4, 8, 16]:\n",
        "            a = torch.randn(m_size, k_size, dtype=torch.float16, device='cuda') / math.sqrt(k_size)\n",
        "            b = torch.randn(k_size, n_size, dtype=torch.float16, device='cuda') / math.sqrt(k_size)\n",
        "            c = torch.empty(m_size, n_size, dtype=torch.float16, device='cuda')\n",
        "\n",
        "            kernel(m_size, n_size, k_size, a, b, c)\n",
        "            torch.testing.assert_close(c, torch.matmul(a, b), rtol=1e-2, atol=1e-2)\n",
        "\n",
        "main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "import math\n",
        "import torch\n",
        "import tilus\n",
        "from tilus import float16, float32, int32\n",
        "from tilus.utils import cdiv\n",
        "\n",
        "\n",
        "class Matmul(tilus.Script):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.block_m = 64\n",
        "        self.block_n = 128\n",
        "        self.block_k = 16\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        m_size: int32,\n",
        "        n_size: int,\n",
        "        k_size: int,\n",
        "        a_ptr: ~float16,\n",
        "        b_ptr: ~float16,\n",
        "        c_ptr: ~float16,\n",
        "    ):\n",
        "        self.attrs.blocks = [\n",
        "            cdiv(m_size, self.block_m),  # the x dimension size of the grid\n",
        "            cdiv(n_size, self.block_n),  # the y dimension size of the grid\n",
        "        ]\n",
        "        self.attrs.warps = 4\n",
        "\n",
        "        offset_m: int32 = self.block_m * self.blockIdx.x\n",
        "        offset_n: int32 = self.block_n * self.blockIdx.y\n",
        "\n",
        "        # create two global tensors `ga` and `gb`\n",
        "        ga = self.global_view(a_ptr, dtype=float16, shape=[m_size, k_size])\n",
        "        gb = self.global_view(b_ptr, dtype=float16, shape=[k_size, n_size])\n",
        "\n",
        "        # create a register tensor `acc` for accumulating the results.\n",
        "        acc = self.register_tensor(dtype=float32, shape=[self.block_m, self.block_n], init=0.0)\n",
        "\n",
        "        # iterate over the k dimension in blocks of size `block_k`.\n",
        "        for k in range(cdiv(k_size, self.block_k)):\n",
        "            # calculate the offset for the current block in the k dimension\n",
        "            offset_k = k * self.block_k\n",
        "\n",
        "            # load a block of matrix A and B into register tensors `a` and `b`.\n",
        "            a = self.load_global(\n",
        "                ga, offsets=[offset_m, offset_k], shape=[self.block_m, self.block_k]\n",
        "            )\n",
        "            b = self.load_global(\n",
        "                gb, offsets=[offset_k, offset_n], shape=[self.block_k, self.block_n]\n",
        "            )\n",
        "\n",
        "            # perform the dot product: acc = a @ b + acc\n",
        "            self.dot(a, b, acc, out=acc)\n",
        "\n",
        "        # after the loop, we cast the accumulated result `acc` to float16 type\n",
        "        acc_f16 = self.cast(acc, dtype=float16)\n",
        "\n",
        "        # store it back to the output matrix C.\n",
        "        gc = self.global_view(c_ptr, dtype=float16, shape=[m_size, n_size])\n",
        "        self.store_global(gc, acc_f16, offsets=[offset_m, offset_n])\n",
        "\n",
        "\n",
        "def main():\n",
        "    kernel = Matmul()\n",
        "\n",
        "    for k_size, n_size in [(4096, 4096), (4096, 12288)]:\n",
        "        for m_size in [1, 4, 8, 16]:\n",
        "            a = torch.randn(m_size, k_size, dtype=torch.float16, device=\"cuda\") / math.sqrt(k_size)\n",
        "            b = torch.randn(k_size, n_size, dtype=torch.float16, device=\"cuda\") / math.sqrt(k_size)\n",
        "            c = torch.empty(m_size, n_size, dtype=torch.float16, device=\"cuda\")\n",
        "\n",
        "            kernel(m_size, n_size, k_size, a, b, c)\n",
        "            torch.testing.assert_close(c, torch.matmul(a, b), rtol=1e-2, atol=1e-2)\n",
        "\n",
        "\n",
        "main()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[Building] matmul-4096-4096-d1: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:09<00:00,  9.37s/it]\r[Building] matmul-4096-4096-d1: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:09<00:00,  9.37s/it]\n",
            "\r[Building] matmul-12288-4096-d1: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:05<00:00,  5.24s/it]\r[Building] matmul-12288-4096-d1: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:05<00:00,  5.24s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Example of a JIT Kernel\n",
        "This example has a `AcceleratorError Cuda Error`.\n",
        "Conceretely, we start by defining M, N, and K,\n",
        "\n",
        "The next step is to create two global tensors, A and B, which are basically just the A and B defined in the matrix multiplication. we have a register tensor that holds the results temporarily to be in register for fast access and write locality. all we have to do is load incrementally in the number of steps of blocks, the matrix A and B.\n",
        "\n",
        "Then we compute the dot product and we keep on repeating this process. \n",
        "\n",
        "In terms of quantization, we cast after the result for precision to float32. This can save us memory. Then we put it into the output matrix, which is not the same as the register tensor. \n",
        "\n",
        "unfortunately, at the time of writing, 16 september 2025, this kernel does have an illegal memory access error on a nvidia l4, which should work. we're also using the right version of cuda python, which should be less than 13, which is a strange bug. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tilus Type System\n",
        " Languages like CUDA are obviously strongly typed. Python is not. It's important for Tilus to be typed in order to have some guarantee of correctness. \n",
        "\n",
        " Three main types in Tilus are Scalar, Pointer, and Tensor, where Tensors simply are groups of scalars that are defined using pointers for each entry (or whatever row/col-major access exists)\n",
        "\n",
        "# Scalars\n",
        " https://nvidia.github.io/tilus/programming-guides/type-system/scalar-types.html\n",
        " It might be interesting to look here to see all of the high- and low-precision floats and integers that exist. \n",
        "\n",
        "\n",
        "# Pointers\n",
        "The syntax is `~<dtype>` where `dtype` is one of the scalar types. Easy hierarchy to understand!\n",
        " You can point to a scalar, a pointer, or to a void pointer, which means you're pointing to a data type you don't know yet, which is quote: a generic pointer type. \n",
        "\n",
        " # Register Tensor\n",
        " This means the tensor is stored in the register, the fast access system. \n",
        " You can define it as such:\n",
        " ```python\n",
        " self.register_tensor(dtype=float32, shape=[32, 64])\n",
        "```\n",
        "\n",
        "It's optional to specify the layout of the register tensor. The shape is simply a tuple of integers specifying the size of each dimension. (Array? The example above is directly from the documentation. )\n",
        "\n",
        "Seek the dedicated explanation for register layout to explain how the tensor elements are distributed among each thread in the thread block. (The 3D element)\n",
        "\n",
        "# Shared Tensor\n",
        " This is stored in the shared memory of the GPU thread block, everybody in shared memory needs to be explicitly allocated and deallocated and before we free it we need to make sure nothing is waiting for it to use in its operations. We cannot directly operate on shared tensors. We need to load shared tensors into the appropriate registers, make the computation, and send it back to shared memory. The reason behind this is simple: speed. In one line, each thread in the thread block can access the shared tensor. Outside of the block, that is not true.  \n",
        " Example:\n",
        "```python\n",
        "self.shared_tensor(dtype=float32, shape=[32, 64])\n",
        "```\n",
        "\n",
        " Again, the shared tensor also has an optional layout parameter to specify the layout of the tensor. A shared layout defines how tensors are stored. The shared layout is a mapping utility from the multi-dimensional shape of the tensor to a linear memory address in the shared memory, for example, row-wise or column-wise, row-major or column-major layout.\n",
        "\n",
        " The shared tensor is shared by all threads in the thread block. Everybody can access them; however, we want to optimize the access patterns and data locality, which is another recurring principle. \n",
        "\n",
        "# Global Tensor\n",
        "\n",
        "```python\n",
        "self.global_tensor(dtype=float32, shape=[32, 64])\n",
        "```\n",
        " The global tensor can be used as a tensor shared by every single thread block in a given kernel. Then the global tensor can be alive for the lifetime of the entire kernel.\n",
        " Just like shared tensors, we don't provide direct memory access. They will be loaded into registered tensors. \n",
        "\n",
        " In a global tensor, you can use layout or strides as optional parameters. Normally, we'll assume row-major, which is a compact form. Otherwise, we can use strides to define the strides of the tensor in each dimension, which is the number of elements to skip in each dimension to get to the next entry in that dimension. This will affect how it's laid out in memory. If your layout parameter is provided, it will have some custom mapping from the 3D indices to the linear memory address. This is used rarely in practice.\n",
        "\n",
        " # Instructions\n",
        " https://nvidia.github.io/tilus/programming-guides/instructions.html\n",
        "\n",
        "Interesting notes:\n",
        " these have basically all of the mathematical methods in elementwise and transforms. We also have load, store, and asynchronous copy instructions. \n",
        "\n",
        " # Instruction Overview\n",
        " Here is an overview of some of the most interesting ones. \n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Autotuning\n",
        "Autotuning is just HPO! Each configuration is called a schedule, and run the different schedules, benchmarking each schedule's performance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "from tilus.ir.layout import spatial, local, visualize_layout\n",
        "print(visualize_layout(local(3, 4)))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/site-packages/tilus/ir/mfunction/ops.py:156: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  \"\"\"Check whether the multi-function fa covers the multi-function fb.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RegisterLayout(shape=[3, 4], mode_shape=[3, 4], spatial_modes=[], local_modes=[0, 1])\n",
            "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
            "\u2502 0: 0 \u2502 0: 1 \u2502 0: 2  \u2502 0: 3  \u2502\n",
            "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
            "\u2502 0: 4 \u2502 0: 5 \u2502 0: 6  \u2502 0: 7  \u2502\n",
            "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
            "\u2502 0: 8 \u2502 0: 9 \u2502 0: 10 \u2502 0: 11 \u2502\n",
            "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Register Layouts (from scratch, with diagrams)\n",
        "==============================================\n",
        "\n",
        "0) Big picture\n",
        "--------------\n",
        "A **register layout** tells you, for every element of a logical tensor owned by a thread block, **which thread(s)** hold that element and **at what local register index within that thread** it lives. This is fundamentally different from global/shared-memory layouts:\n",
        "\n",
        "- **Global / shared layout**: \u201cGiven a tensor index `(i, j, \u2026)`, what is the byte address in global or shared memory?\u201d\n",
        "- **Register layout**: \u201cGiven a tensor index `(i, j, \u2026)`, which **thread id(s)** store it, and what is the **local register slot** inside each such thread?\u201d\n",
        "\n",
        "Concretely, a register layout is a mapping:\n",
        "\n",
        "    (thread_id, local_id)  <->  logical_index  (i.e., (i, j, ..., in d dimensions))\n",
        "\n",
        "The mapping is many-to-one in general, because multiple threads can legally hold replicas of the same logical element.\n",
        "\n",
        "\n",
        "1) The two essential kinds of modes\n",
        "-----------------------------------\n",
        "We factor the tensor\u2019s shape into **modes** (sub-dimensions). Each mode is then assigned to one of two disjoint categories:\n",
        "\n",
        "- **Spatial modes** \u2192 decide **which thread** gets the element. They index over parallel workers.\n",
        "- **Local modes**   \u2192 decide **where inside the thread\u2019s local register array** the element sits.\n",
        "\n",
        "This split is the heart of the system. One mode cannot be both spatial and local. Violating that creates contradictions and is disallowed.\n",
        "\n",
        "We track four attributes:\n",
        "- `shape`: the overall tensor shape (must match the tensor this layout is for).\n",
        "- `mode_shape`: the sizes of all sub-dimensions after optional splits. Modes of size `1` are pruned.\n",
        "- `spatial_modes`: an ordered list of mode indices used to compute `thread_id`.\n",
        "- `local_modes`:   an ordered list of mode indices used to compute `local_id`.\n",
        "\n",
        "Order matters in both lists; it defines the linearization order (row-major over the listed modes).\n",
        "\n",
        "\n",
        "2) From raw tensor shape to `mode_shape`\n",
        "----------------------------------------\n",
        "You may optionally **split** each tensor dimension into multiple **modes**. Example:\n",
        "\n",
        "- Tensor shape `[3, 4]`.\n",
        "  - Keep the first dimension as is: mode size `3`.\n",
        "  - Split the second dimension `4` into `2 \u00d7 2`.\n",
        "  - Resulting `mode_shape = [3, 2, 2]`.\n",
        "\n",
        "Another example:\n",
        "\n",
        "- Tensor shape `[12, 1, 6]`.\n",
        "  - Split `12` into `3 \u00d7 4`.\n",
        "  - Keep `1` as is (it will be pruned later).\n",
        "  - Split `6` into `2 \u00d7 3`.\n",
        "  - Raw mode shape `[3, 4, 1, 2, 3]` \u2192 prune ones \u2192 `mode_shape = [3, 4, 2, 3]`.\n",
        "\n",
        "Intuition: you are factorizing a mixed-radix indexer. This gives you dials you can assign to \u201cwhich thread?\u201d versus \u201cwhich slot inside the thread?\u201d\n",
        "\n",
        "\n",
        "3) How a layout answers the central question\n",
        "--------------------------------------------\n",
        "**Given a logical tensor index `(i, j, \u2026)`**, the layout determines:\n",
        "- the corresponding **mode indices** (one per entry in `mode_shape`);\n",
        "- the subset of those mode indices that are **spatial**, in the order specified by `spatial_modes`;\n",
        "- the subset that are **local**, in the order specified by `local_modes`;\n",
        "- a row-major linearization of the spatial subset \u2192 `thread_id`;\n",
        "- a row-major linearization of the local subset \u2192 `local_id`.\n",
        "\n",
        "Pseudocode (normal, non-replicated case):\n",
        "\n",
        "```\n",
        "\n",
        "# Inputs:\n",
        "\n",
        "# shape           ... tensor shape (for bounds)\n",
        "\n",
        "# mode\\_shape      ... e.g., \\[m0, m1, m2, ...]\n",
        "\n",
        "# spatial\\_modes   ... e.g., \\[s0, s1, ...]   (indices into mode\\_shape)\n",
        "\n",
        "# local\\_modes     ... e.g., \\[l0, l1, ...]   (indices into mode\\_shape)\n",
        "\n",
        "# idx             ... logical tensor index (multidim), e.g., (i, j, ...)\n",
        "\n",
        "# Step A: map idx -> mode\\_index using mixed-radix decomposition:\n",
        "\n",
        "# Flatten idx to a single linear p in row-major over 'shape', then unflatten over 'mode\\_shape'.\n",
        "\n",
        "mode\\_index = unflatten\\_over\\_mode\\_shape(flatten\\_over\\_shape(idx, shape), mode\\_shape)\n",
        "\n",
        "# mode\\_index has length len(mode\\_shape)\n",
        "\n",
        "# Step B: pick out spatial and local sub-tuples in the requested order:\n",
        "\n",
        "spatial\\_index = \\[ mode\\_index\\[k] for k in spatial\\_modes ]\n",
        "local\\_index   = \\[ mode\\_index\\[k] for k in local\\_modes ]\n",
        "\n",
        "# Step C: row-major linearization helpers:\n",
        "\n",
        "def row\\_major\\_linear(index\\_tuple, shape\\_tuple):\n",
        "\\# index\\_tuple and shape\\_tuple have same length\n",
        "acc = 0\n",
        "for t in range(len(index\\_tuple)):\n",
        "acc = acc \\* shape\\_tuple\\[t] + index\\_tuple\\[t]\n",
        "return acc\n",
        "\n",
        "spatial\\_shape = \\[ mode\\_shape\\[k] for k in spatial\\_modes ]\n",
        "local\\_shape   = \\[ mode\\_shape\\[k] for k in local\\_modes ]\n",
        "\n",
        "thread\\_id = row\\_major\\_linear(spatial\\_index, spatial\\_shape)\n",
        "local\\_id  = row\\_major\\_linear(local\\_index,   local\\_shape)\n",
        "\n",
        "```\n",
        "\n",
        "This is exactly what your worked example in \u00a78.3.5.1 does with `mode_shape=[2,2,3,2]`, `spatial_modes=[0,2]`, and `local_modes=[3,1]`:\n",
        "- `spatial_index = [i//2, j//2]`, shaped `[2,3]` \u2192 `thread_id = (i//2)*3 + (j//2)`.\n",
        "- `local_index   = [j%2,  i%2]`, shaped `[2,2]` \u2192 `local_id  = (j%2)*2 + (i%2)`.\n",
        "\n",
        "ASCII diagram of the idea (each box = one logical element):\n",
        "```\n",
        "\n",
        "+----------------------------------+\n",
        "\\| logical tensor (shape H x W)     |\n",
        "\\|   split into modes (mode\\_shape)  |\n",
        "+-------------------+--------------+\n",
        "|\n",
        "v\n",
        "\\[split into modes]\n",
        "mode indices ---> pick & order ---> \\[spatial modes] --row-major--> thread\\_id\n",
        "-> pick & order ---> \\[local modes]   --row-major--> local\\_id\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "4) Two trivial layouts to build intuition\n",
        "-----------------------------------------\n",
        "4.1) **Local-only** layout: everything lives in **one thread**; that one thread stores all elements in row-major order:\n",
        "\n",
        "```\n",
        "\n",
        "layout = local(3, 4)   # shape = (3,4); spatial\\_modes=\\[], local\\_modes=\\[0,1]\n",
        "\n",
        "Grid diagram (each cell shows \"thread\\_id : local\\_id\"):\n",
        "\n",
        "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
        "\u2502 0: 0 \u2502 0: 1 \u2502 0: 2  \u2502 0: 3  \u2502\n",
        "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
        "\u2502 0: 4 \u2502 0: 5 \u2502 0: 6  \u2502 0: 7  \u2502\n",
        "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
        "\u2502 0: 8 \u2502 0: 9 \u2502 0: 10 \u2502 0: 11 \u2502\n",
        "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
        "\n",
        "```\n",
        "\n",
        "4.2) **Purely spatial** layout: each element goes to a distinct thread; each thread stores exactly one element:\n",
        "\n",
        "```\n",
        "\n",
        "layout = spatial(3, 2)   # shape = (3,2); spatial\\_modes=\\[0,1], local\\_modes=\\[]\n",
        "\n",
        "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
        "\u2502 0: 0 \u2502 1: 0 \u2502\n",
        "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
        "\u2502 2: 0 \u2502 3: 0 \u2502\n",
        "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
        "\u2502 4: 0 \u2502 5: 0 \u2502\n",
        "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
        "\n",
        "```\n",
        "\n",
        "These are opposite ends of the spectrum. Real kernels interleave both kinds of modes to balance occupancy, register pressure, coalescing, and math/tensor-core feed rates.\n",
        "\n",
        "\n",
        "5) Composition: replacing each element with a tile\n",
        "--------------------------------------------------\n",
        "**Composition** says: \u201ctake each element of an outer layout, and **replace it** with a whole tensor (an inner layout).\u201d Dimensions multiply; mode lists concatenate with adjusted indices.\n",
        "\n",
        "Composition is **associative** (grouping does not matter) but **not commutative** (order matters).\n",
        "\n",
        "5.1) Example A \u2014 `local(3,4).spatial(2,3)`:\n",
        "\n",
        "Interpretation: start from one-thread local storage of a 3\u00d74 tile; then fan each element out spatially over `(2,3)` threads.\n",
        "\n",
        "Result shape: `(3*2, 4*3) = (6,12)`.\n",
        "\n",
        "```\n",
        "\n",
        "RegisterLayout(shape=\\[6, 12], mode\\_shape=\\[3, 2, 4, 3],\n",
        "spatial\\_modes=\\[1, 3], local\\_modes=\\[0, 2])\n",
        "\n",
        "\\[Partial grid excerpt; each cell shows \"thread\\_id : local\\_id\"]\n",
        "\n",
        "Row 0:  0:0  1:0  2:0   0:1  1:1  2:1   0:2  1:2  2:2   0:3  1:3  2:3\n",
        "Row 1:  3:0  4:0  5:0   3:1  4:1  5:1   3:2  4:2  5:2   3:3  4:3  5:3\n",
        "...\n",
        "\n",
        "```\n",
        "\n",
        "Here, each original \u201clocal element\u201d became a 2\u00d73 block spread across 6 threads, but the local tiling `[3,4]` is still present as part of `local_id`.\n",
        "\n",
        "5.2) Example B \u2014 `spatial(2,3).local(3,4)`:\n",
        "\n",
        "Inverse order: first spread over 6 threads, then inside each thread store a 3\u00d74 tile. The global shape is the same `(6,12)`, but the mapping is different:\n",
        "\n",
        "```\n",
        "\n",
        "RegisterLayout(shape=\\[6, 12], mode\\_shape=\\[2, 3, 3, 4],\n",
        "spatial\\_modes=\\[0, 2], local\\_modes=\\[1, 3])\n",
        "\n",
        "Top-left 3\u00d74 is entirely in thread 0 with local\\_ids 0..11,\n",
        "next 3\u00d74 is in thread 1, etc. Different distribution than Example A.\n",
        "\n",
        "```\n",
        "\n",
        "ASCII contrast:\n",
        "\n",
        "```\n",
        "\n",
        "local(3,4).spatial(2,3):            spatial(2,3).local(3,4):\n",
        "\\[local first, then scatter]         \\[scatter first, then local tiles]\n",
        "\n",
        "```\n",
        "\n",
        "Composition is the workhorse for building complex MMA-friendly layouts out of small, clear pieces.\n",
        "\n",
        "\n",
        "6) Tensor Core (PTX MMA) operand layouts\n",
        "----------------------------------------\n",
        "PTX manuals document register layouts for MMA operands. Those are precisely **register** layouts in this framework.\n",
        "\n",
        "Example: operand **C** for `mma.sync.aligned.m16n8k8.f16,f16,f16,f16`.\n",
        "\n",
        "One concise construction:\n",
        "\n",
        "```\n",
        "\n",
        "layout = repeat(2, 1).spatial(8, 4).repeat(1, 2)\n",
        "\n",
        "RegisterLayout(shape=\\[16, 8],\n",
        "mode\\_shape=\\[2, 8, 4, 2],\n",
        "spatial\\_modes=\\[1, 2],\n",
        "local\\_modes=\\[0, 3])\n",
        "\n",
        "```\n",
        "\n",
        "Explanation:\n",
        "\n",
        "- `spatial(8,4)` selects the 32 participating threads (8\u00d74) and determines **which** thread holds each C element.\n",
        "- The two `repeat(...)` wrappers add **local replication modes** of sizes 2 and 2 respectively, yielding local register indices `[0..3]` per thread for the fragment.\n",
        "- The final mode order (`spatial_modes=[1,2]`, `local_modes=[0,3]`) matches the published mapping.\n",
        "\n",
        "Visual pattern (each cell shows \u201ct : r\u201d for thread and local register index):\n",
        "\n",
        "```\n",
        "\n",
        "Row 0..7:   threads 0..31 with local regs 0..1\n",
        "Row 8..15:  threads 0..31 with local regs 2..3\n",
        "\n",
        "```\n",
        "\n",
        "You can construct the standard A/B/C operand tilings the same way by composing `spatial`, `local`, and `repeat` blocks in the order the PTX figures imply.\n",
        "\n",
        "\n",
        "7) Allowing multiple threads to hold the same element\n",
        "-----------------------------------------------------\n",
        "Some algorithms need **replication**: multiple threads must read/update the same logical element. The layout explicitly supports this by letting **spatial modes** include a **replication mode**, represented as a **negative size**: `-R` means \u201creplicate R times\u201d.\n",
        "\n",
        "Example:\n",
        "\n",
        "```\n",
        "\n",
        "spatial(3,4):\n",
        "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
        "\u2502 0:0  \u2502 1:0  \u2502 2:0   \u2502 3:0   \u2502\n",
        "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
        "\u2502 4:0  \u2502 5:0  \u2502 6:0   \u2502 7:0   \u2502\n",
        "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
        "\u2502 8:0  \u2502 9:0  \u2502 10:0  \u2502 11:0  \u2502\n",
        "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
        "\n",
        "```\n",
        "\n",
        "Reducing over the first dimension (collapsing rows) produces **replication**:\n",
        "\n",
        "```\n",
        "\n",
        "reduce(spatial(3,4), dims=\\[0])  \u21d2  shape=\\[4], spatial\\_modes=\\[-3, 0]\n",
        "\n",
        "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
        "\u2502 \\[0,4,8]:0      \u2502 \\[1,5,9]:0      \u2502 \\[2,6,10]:0      \u2502 \\[3,7,11]:0      \u2502\n",
        "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
        "\n",
        "Meaning: the logical column-0 element is held by threads {0,4,8} at local\\_id 0, etc.\n",
        "\n",
        "```\n",
        "\n",
        "In mapping terms, a replicated spatial mode yields **multiple** `thread_id` results for one logical index. Local indices stay the same; you just return a set of `(thread_id, local_id)` pairs.\n",
        "\n",
        "\n",
        "8) Column-major vs row-major local/spatial orders\n",
        "-------------------------------------------------\n",
        "Changing the **order** of modes in `local_modes` reorders registers inside each thread. Changing the order in `spatial_modes` permutes which thread owns which element.\n",
        "\n",
        "Row-major local:\n",
        "\n",
        "```\n",
        "\n",
        "local(2,3): local\\_modes=\\[0,1]\n",
        "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
        "\u2502 0:0  \u2502 0:1  \u2502 0:2  \u2502\n",
        "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
        "\u2502 0:3  \u2502 0:4  \u2502 0:5  \u2502\n",
        "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
        "\n",
        "```\n",
        "\n",
        "Column-major local:\n",
        "\n",
        "```\n",
        "\n",
        "column\\_local(2,3): local\\_modes=\\[1,0]\n",
        "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
        "\u2502 0:0  \u2502 0:2  \u2502 0:4  \u2502\n",
        "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
        "\u2502 0:1  \u2502 0:3  \u2502 0:5  \u2502\n",
        "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
        "\n",
        "```\n",
        "\n",
        "Row-major spatial:\n",
        "\n",
        "```\n",
        "\n",
        "spatial(2,3): spatial\\_modes=\\[0,1]\n",
        "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
        "\u2502 0:0  \u2502 1:0  \u2502 2:0  \u2502\n",
        "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
        "\u2502 3:0  \u2502 4:0  \u2502 5:0  \u2502\n",
        "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
        "\n",
        "```\n",
        "\n",
        "Column-major spatial:\n",
        "\n",
        "```\n",
        "\n",
        "column\\_spatial(2,3): spatial\\_modes=\\[1,0]\n",
        "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
        "\u2502 0:0  \u2502 2:0  \u2502 4:0  \u2502\n",
        "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
        "\u2502 1:0  \u2502 3:0  \u2502 5:0  \u2502\n",
        "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
        "\n",
        "```\n",
        "\n",
        "These differences matter for vectorization, coalescing, and matching hardware operand swizzles.\n",
        "\n",
        "\n",
        "9) The mapping process: full algorithm with replication\n",
        "-------------------------------------------------------\n",
        "We extend the earlier pseudocode to handle **replicated spatial modes**. A replicated mode has size `-R` (negative). Treat that as \u201cR choices of a replication lane\u201d that **do not** change the logical index, only which thread copy you pick.\n",
        "\n",
        "```python\n",
        "\n",
        "def apply\\_layout(shape, mode\\_shape, spatial\\_modes, local\\_modes, idx):\n",
        "\\# 1) idx -> mode\\_index via mixed radix decomposition:\n",
        "p = flatten\\_over\\_shape(idx, shape)              # linear in row-major over 'shape'\n",
        "mode\\_index = unflatten\\_over\\_mode\\_shape(p, mode\\_shape)\n",
        "\n",
        "\n",
        "# 2) Gather local indices and shapes in order:\n",
        "local_idx   = [mode_index[k] for k in local_modes]\n",
        "local_shape = [abs(mode_shape[k]) for k in local_modes]\n",
        "local_id    = row_major_linear(local_idx, local_shape)\n",
        "\n",
        "# 3) Gather spatial indices. Separate normal vs replicated:\n",
        "spatial_idx_vals = []\n",
        "spatial_shape    = []\n",
        "rep_factors      = []   # collect replication sizes\n",
        "\n",
        "for k in spatial_modes:\n",
        "    sz = mode_shape[k]\n",
        "    if sz > 0:\n",
        "        spatial_idx_vals.append(mode_index[k])\n",
        "        spatial_shape.append(sz)\n",
        "    else:\n",
        "        rep_factors.append(-sz)  # replication of size R = -sz\n",
        "\n",
        "base_thread_id = row_major_linear(spatial_idx_vals, spatial_shape)\n",
        "\n",
        "# 4) If replicated, return R copies with offsets over a replication lane.\n",
        "# The actual placement of replication lanes in thread_id space is implementation-defined.\n",
        "# A simple scheme: treat replication lanes as a leading product factor.\n",
        "if not rep_factors:\n",
        "    return [(base_thread_id, local_id)]\n",
        "else:\n",
        "    R = 1\n",
        "    for r in rep_factors: R *= r\n",
        "    # produce R copies; lane_id in [0..R-1]\n",
        "    return [ (lane_id * total_threads_per_nonrep + base_thread_id, local_id)\n",
        "             for lane_id in range(R) ]\n",
        "\n",
        "# Note: 'total\\_threads\\_per\\_nonrep' depends on how the runtime packs replication lanes into thread\\_id space.\n",
        "\n",
        "# Frameworks may interleave or block them; the semantics are \"R identical copies exist\", not the exact numbering.\n",
        "```\n",
        "\n",
        "In plain language: compute the usual `thread_id` from normal spatial modes; then fan that out across replication lanes. Each fan-out corresponds to \u201cthe same logical element appears in multiple threads.\u201d\n",
        "\n",
        "\n",
        "10) Operations provided by the layout module\n",
        "--------------------------------------------\n",
        "10.1) **Creation**\n",
        "- `spatial(*shape[, ranks])` \u2014 make a purely spatial layout (every element is owned by some thread; no local tiling).\n",
        "- `local(*shape[, ranks])` \u2014 make a purely local layout (single thread; everything in registers).\n",
        "- `column_spatial(*shape)` \u2014 same as `spatial` but mode order is column-major (i.e., reversed for 2D).\n",
        "- `column_local(*shape)` \u2014 same as `local` but local mode order is column-major.\n",
        "- `auto_local_spatial(num_threads, shape)` \u2014 produce a `local(...).spatial(...)` that uses the given number of threads for the given tensor `shape`.\n",
        "\n",
        "10.2) **Transformation** (do **not** change \u201cthreads per element\u201d and \u201celements per thread\u201d; they retile the same resources)\n",
        "- `squeeze(layout, dims)` \u2014 remove size-1 dimensions in `shape` and the corresponding modes.\n",
        "- `unsqueeze(layout, dims)` \u2014 insert size-1 dimensions.\n",
        "- `permute(layout, dims)` \u2014 reorder tensor dimensions (and remap modes consistently).\n",
        "- `reshape(layout, shape)` \u2014 change `shape` while preserving element count; redistributes mode factors accordingly.\n",
        "- `flatten(layout[, start_dim, end_dim])` \u2014 merge a range of dimensions into one.\n",
        "\n",
        "10.3) **Composition**\n",
        "- `concat(lhs, rhs)` \u2014 concatenate layouts along a dimension (grow shape; threads/elements per thread unchanged per tile).\n",
        "- `compose(outer, inner)` \u2014 replacement rule: each element of `outer` becomes a whole `inner` tile; shapes multiply; spatial/local mode lists combine.\n",
        "\n",
        "10.4) **Other**\n",
        "- `divide(lhs, rhs)` \u2014 inverse of certain compositions when factorizations match (split a layout into outer/inner components).\n",
        "- `reduce(layout, dims[, keepdims])` \u2014 remove dimensions by combining them; if you reduce away a spatial dimension, you typically introduce **replication** (negative spatial mode sizes).\n",
        "\n",
        "\n",
        "11) Worked example mirroring \u00a78.3.5.1\n",
        "-------------------------------------\n",
        "Given:\n",
        "- `shape = [4, 6]`\n",
        "- `mode_shape = [2, 2, 3, 2]`\n",
        "- `spatial_modes = [0, 2]`\n",
        "- `local_modes   = [3, 1]`\n",
        "\n",
        "For a logical index `(i, j)`:\n",
        "\n",
        "- Compute `mode_index = [ i//2, i%2, j//2, j%2 ]`.\n",
        "- Spatial subtuple (in listed order) is `[ i//2, j//2 ]` with shape `[2, 3]`.\n",
        "  \u2192 `thread_id = (i//2) * 3 + (j//2)`.\n",
        "- Local subtuple   (in listed order) is `[ j%2,  i%2 ]` with shape `[2, 2]`.\n",
        "  \u2192 `local_id  = (j%2) * 2 + (i%2)`.\n",
        "\n",
        "This gives an unambiguous, fast, index-to-(thread,register) mapping suitable for kernel code generation.\n",
        "\n",
        "\n",
        "12) Inverse mapping (from a location back to a logical index)\n",
        "-------------------------------------------------------------\n",
        "When an element is **not replicated**, the mapping is bijective from `(thread_id, local_id)` to a unique logical index.\n",
        "\n",
        "Sketch:\n",
        "\n",
        "```\n",
        "\n",
        "# Given thread\\_id, invert spatial linearization to get spatial\\_index tuple:\n",
        "\n",
        "spatial\\_index = unlinearize\\_row\\_major(thread\\_id, spatial\\_shape)\n",
        "\n",
        "# Given local\\_id, invert local linearization:\n",
        "\n",
        "local\\_index   = unlinearize\\_row\\_major(local\\_id,   local\\_shape)\n",
        "\n",
        "# Now fill a full mode\\_index\\[] of length len(mode\\_shape):\n",
        "\n",
        "for t, k in enumerate(spatial\\_modes): mode\\_index\\[k] = spatial\\_index\\[t]\n",
        "for t, k in enumerate(local\\_modes):   mode\\_index\\[k] = local\\_index\\[t]\n",
        "\n",
        "# Finally, re-linearize over mode\\_shape, then unflatten over the original 'shape':\n",
        "\n",
        "p = linearize\\_over\\_mode\\_shape(mode\\_index, mode\\_shape)\n",
        "idx = unflatten\\_over\\_shape(p, shape)\n",
        "\n",
        "```\n",
        "\n",
        "With replication present, multiple `(thread_id, local_id)` pairs map to the **same** logical index; the inverse returns that same `idx` regardless of which replica you use.\n",
        "\n",
        "\n",
        "13) Practical guidance and common pitfalls\n",
        "------------------------------------------\n",
        "- Do not assign the same mode to both `spatial_modes` and `local_modes`. This breaks the definition; the implementation forbids it.\n",
        "- Pay attention to **mode order**. Changing `[0,1]` to `[1,0]` is not cosmetic; it changes stride/contiguity and can make or break coalescing and vectorized register loads.\n",
        "- Use **composition** to mirror hardware conventions (e.g., tensor core operand fragments). Read the PTX figure, then build it with `spatial`, `local`, and `repeat` in the same logical order.\n",
        "- Use **reduce** to introduce **replication** when several threads must share an element (e.g., block-wide broadcast without shared-memory staging).\n",
        "- Remember that **transformations** (`reshape`, `permute`, `flatten`, `squeeze/unsqueeze`) **keep** the count of threads and registers per thread; they only retile how those resources are mapped onto the tensor\u2019s shape.\n",
        "\n",
        "\n",
        "Appendix: Minimal ASCII \u201ctile replacement\u201d illustration\n",
        "-------------------------------------------------------\n",
        "Take an outer 2\u00d72 layout. Replace every cell with an inner 2\u00d73 tile.\n",
        "\n",
        "Outer:\n",
        "\u250c\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2510\n",
        "\u2502 A \u2502 B \u2502\n",
        "\u251c\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2524\n",
        "\u2502 C \u2502 D \u2502\n",
        "\u2514\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2518\n",
        "\n",
        "Inner 2\u00d73 (for each letter):\n",
        "\u250c\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2510\n",
        "\u2502 a \u2502 b \u2502 c \u2502\n",
        "\u251c\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2524\n",
        "\u2502 d \u2502 e \u2502 f \u2502\n",
        "\u2514\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2518\n",
        "\n",
        "Composition (outer \u2218 inner) gives a 4\u00d76:\n",
        "\u250c\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2510\n",
        "\u2502Aa \u2502Ab \u2502Ac \u2502Ba \u2502Bb \u2502Bc \u2502\n",
        "\u251c\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2524\n",
        "\u2502Ad \u2502Ae \u2502Af \u2502Bd \u2502Be \u2502Bf \u2502\n",
        "\u251c\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2524\n",
        "\u2502Ca \u2502Cb \u2502Cc \u2502Da \u2502Db \u2502Dc \u2502\n",
        "\u251c\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2524\n",
        "\u2502Cd \u2502Ce \u2502Cf \u2502Dd \u2502De \u2502Df \u2502\n",
        "\u2514\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2518\n",
        "\n",
        "If the outer is `local(...)` and inner is `spatial(...)`, letters map to local slots first, then to threads. Reverse the order and you reverse the ownership pattern.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "from tilus import float16, float32, int32\n",
        "from tilus.utils import cdiv\n",
        "\n",
        "\n",
        "class MatmulV0(tilus.Script):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # we define three hyperparameters: ``block_m``, ``block_n``, and ``block_k`` to determine the tile size on\n",
        "        # m, n, and k dimensions for each `thread block` of the kernel.\n",
        "        self.block_m = 64\n",
        "        self.block_n = 64\n",
        "        self.block_k = 16\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        m_size: int32,  # the size of the m dimension of the input matrix A and output matrix C\n",
        "        n_size: int,  # the size of the n dimension of the input matrix B and output matrix C\n",
        "        k_size: int,  # the size of the k dimension of the input matrix A and B\n",
        "        a_ptr: ~float16,  # the pointer to the input matrix A, which is a 2D tensor of shape [m_size, k_size]\n",
        "        b_ptr: ~float16,  # the pointer to the input matrix B, which is a 2D tensor of shape [k_size, n_size]\n",
        "        c_ptr: ~float16,  # the pointer to the output matrix C, which is a 2D tensor of shape [m_size, n_size]\n",
        "    ):\n",
        "        self.attrs.blocks = [\n",
        "            cdiv(m_size, self.block_m),  # the x dimension size of the grid\n",
        "            cdiv(n_size, self.block_n),  # the y dimension size of the grid\n",
        "        ]\n",
        "        self.attrs.warps = (\n",
        "            1  # the number of warps per thread block, must be a compile-time known integer\n",
        "        )\n",
        "\n",
        "        # define two int32 variables to store the offsets of the m and n dimensions for the current thread block.\n",
        "        offset_m: int32 = self.block_m * self.blockIdx.x\n",
        "        offset_n: int32 = self.block_n * self.blockIdx.y\n",
        "\n",
        "        # create two global tensors `ga` and `gb` to represent the input matrices A and B, respectively.\n",
        "        ga = self.global_view(a_ptr, dtype=float16, shape=[m_size, k_size])\n",
        "        gb = self.global_view(b_ptr, dtype=float16, shape=[k_size, n_size])\n",
        "\n",
        "        # create a register tensor `acc` to accumulate the results of the matrix multiplication.\n",
        "        acc = self.register_tensor(dtype=float32, shape=[self.block_m, self.block_n], init=0.0)\n",
        "\n",
        "        # iterate over the k dimension in blocks of size `block_k`.\n",
        "        for k in range(cdiv(k_size, self.block_k)):\n",
        "            # calculate the offset for the current block in the k dimension\n",
        "            offset_k = k * self.block_k\n",
        "\n",
        "            # load a block of matrix A and B into register tensors `a` and `b`.\n",
        "            a = self.load_global(\n",
        "                ga, offsets=[offset_m, offset_k], shape=[self.block_m, self.block_k]\n",
        "            )\n",
        "            b = self.load_global(\n",
        "                gb, offsets=[offset_k, offset_n], shape=[self.block_k, self.block_n]\n",
        "            )\n",
        "\n",
        "            # perform the dot product: acc = a @ b + acc\n",
        "            self.dot(a, b, acc, out=acc)\n",
        "\n",
        "        # after the loop, we cast the accumulated result `acc` to float16 type and store it back to the output matrix C.\n",
        "        acc_f16 = self.cast(acc, dtype=float16)\n",
        "        gc = self.global_view(c_ptr, dtype=float16, shape=[m_size, n_size])\n",
        "        self.store_global(gc, acc_f16, offsets=[offset_m, offset_n])\n",
        "\n",
        "\n",
        "import pandas\n",
        "import torch\n",
        "from tilus.utils import benchmark_func\n",
        "\n",
        "\n",
        "def main():\n",
        "    headers = [\"m\", \"n\", \"k\", \"name\", \"latency (ms)\", \"tflops\"]\n",
        "    workloads = [[4096, 4096, 4096]]\n",
        "\n",
        "    rows = []\n",
        "    for m, n, k in workloads:\n",
        "        # create an instance of the kernel we have just defined\n",
        "        matmul = MatmulV0()\n",
        "\n",
        "        a = (torch.rand(m, k, dtype=torch.float16).cuda() - 0.5) / math.sqrt(k)\n",
        "        b = (torch.rand(k, n, dtype=torch.float16).cuda() - 0.5) / math.sqrt(k)\n",
        "        c_actual = torch.empty(m, n, dtype=torch.float16).cuda()\n",
        "        c_expect = a @ b\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "        # launch the kernel by passing required arguments\n",
        "        matmul(m, n, k, a, b, c_actual)\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "        # check correctness\n",
        "        torch.testing.assert_close(c_expect, c_actual, atol=1e-2, rtol=1e-2)\n",
        "\n",
        "        # benchmark\n",
        "        for name, func in [\n",
        "            (\"torch\", lambda: torch.matmul(a, b, out=c_expect)),\n",
        "            (\"tilus\", lambda: matmul(m, n, k, a, b, c_actual)),\n",
        "        ]:\n",
        "            latency = benchmark_func(func, warmup=5, repeat=20)\n",
        "            tflops = 2 * m * n * k / latency * 1e-9\n",
        "            rows.append([m, n, k, name, latency, tflops])\n",
        "\n",
        "    df = pandas.DataFrame(rows, columns=headers)\n",
        "    print(df)\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[Building] matmul_v0-4096-4096-d1: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:06<00:00,  6.05s/it]\r[Building] matmul_v0-4096-4096-d1: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:06<00:00,  6.05s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      m     n     k   name  latency (ms)     tflops\n",
            "0  4096  4096  4096  torch       1.84832  74.358852\n",
            "1  4096  4096  4096  tilus       4.42880  31.033000\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}